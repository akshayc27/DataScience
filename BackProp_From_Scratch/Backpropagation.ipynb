{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y871RZtRhCEH"
   },
   "source": [
    "# <font color='red'>Backpropagation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0huUQ0byiI0I"
   },
   "source": [
    "## <font color='red'>Loading data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1hSje5CBgcUb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 6)\n",
      "(506, 5) (506,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print(data.shape)\n",
    "X = data[:, :5]\n",
    "y = data[:, -1]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2879095 , -0.12001342, -1.45900038, -0.66660821, -0.14421743,\n",
       "         1.85884913],\n",
       "       [-0.59338101,  0.36716642, -0.30309415, -0.98732948, -0.74026221,\n",
       "         1.81901037],\n",
       "       [-0.59338101, -0.26581176, -0.30309415, -0.98732948, -0.74026221,\n",
       "         1.81989281],\n",
       "       ...,\n",
       "       [ 0.11573841,  0.79744934,  1.17646583, -0.80321172,  0.15812412,\n",
       "         1.59974823],\n",
       "       [ 0.11573841,  0.73699637,  1.17646583, -0.80321172,  0.15812412,\n",
       "         1.5997457 ],\n",
       "       [ 0.11573841,  0.43473151,  1.17646583, -0.80321172,  0.15812412,\n",
       "         1.59957717]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2879095 , -0.12001342, -1.45900038, -0.66660821, -0.14421743])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.858849127371369"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JL-0soQistC"
   },
   "source": [
    "# <font color='red'>Computational graph</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nREnTTJ3i0Vd"
   },
   "source": [
    "<img src='https://i.imgur.com/seSGbNS.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSPX_H_4i_HT"
   },
   "source": [
    "\n",
    "*  **If you observe the graph, we are having input features [f1, f2, f3, f4, f5] and 9 weights [w1, w2, w3, w4, w5, w6,    w7, w8, w9]**.<br><br>\n",
    "*  **The final output of this graph is a value L which is computed as (Y-Y')^2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D54eDEv6jkO4"
   },
   "source": [
    "## <font color='red'> Implementing Forward propagation, Backpropagation and Gradient checking </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwEcPWLffTKI"
   },
   "source": [
    "## <font color='blue'>Forward propagation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCZrm-gkfTKI"
   },
   "source": [
    "\n",
    "*  <b>\n",
    "    Forward propagation</b>(Write your code in<font color='blue'> def forward_propagation()</b></font>)<br><br>\n",
    "    For easy debugging, we will break the computational graph into 3 parts.\n",
    "\n",
    "    <font color='green'><b>Part 1</b></font></b>\n",
    "    <img src='https://i.imgur.com/0xUaxy6.png'><br><br>\n",
    "    <font color='green'><b>Part 2</b></font></b><br>\n",
    "    <img src='https://i.imgur.com/J29pAJL.png'><br><br>\n",
    "    <font color='green'><b>Part 3</b></font></b>\n",
    "    <img src='https://i.imgur.com/vMyCsd9.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ADCovl2FfTKJ"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''In this function, we will compute the sigmoid(z)'''\n",
    "    # we can use this function in forward and backward propagation\n",
    "    # write the code to compute the sigmoid value of z and return that value \n",
    "    val = 1 / (1 + math.exp(-z))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dLJ-OYwefTKJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  #if you have written the code correctly then the grader function will output true\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Symbol, Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KD3piNkifTKJ"
   },
   "outputs": [],
   "source": [
    "y_hat = 0\n",
    "part_3 = 0\n",
    "def forward_propagation(x, y, w):\n",
    "        '''In this function, we will compute the forward propagation '''\n",
    "        # X: input data point, note that in this assignment you are having 5-d data points\n",
    "        # y: output varible\n",
    "        # W: weight array, its of length 9, W[0] corresponds to w1 in graph, W[1] corresponds to w2 in graph,..., W[8] corresponds to w9 in graph.  \n",
    "        # you have to return the following variables\n",
    "        # exp= part1 (compute the forward propagation until exp and then store the values in exp)\n",
    "        # tanh =part2(compute the forward propagation until tanh and then store the values in tanh)\n",
    "        # sig = part3(compute the forward propagation until sigmoid and then store the values in sig)\n",
    "        # we are computing one of the values for better understanding\n",
    "        \n",
    "        val_1= (w[0]*x[0]+w[1]*x[1]) * (w[0]*x[0]+w[1]*x[1]) + w[5]\n",
    "        part_1 = np.exp(val_1)\n",
    "        \n",
    "        val_2 = (part_1 + w[6])\n",
    "        part_2 = math.tanh(val_2)\n",
    "        \n",
    "        val_3 = math.sin(w[2]*x[2]) * ((w[3] * x[3]) + (w[4] * x[4])) + w[7]\n",
    "        part_3 = sigmoid(val_3)\n",
    "        \n",
    "        y_hat = part_2 + (part_3 * w[8])\n",
    "        \n",
    "        dy_pred = (-2) * (y - y_hat)\n",
    "        \n",
    "    \n",
    "        Loss = (y - y_hat) ** 2\n",
    "        \n",
    "        # after computing part1,part2 and part3 compute the value of y' from the main Computational graph using required equations\n",
    "        # write code to compute the value of L=(y-y')^2 and store it in variable loss\n",
    "        # compute derivative of L  w.r.to y' and store it in dy_pred \n",
    "        # Create a dictionary to store all the intermediate values i.e. dy_pred ,loss,exp,tanh,sigmoid\n",
    "        # we will be using the dictionary to find values in backpropagation, you can add other keys in dictionary as well\n",
    "        \n",
    "        forward_dict={}\n",
    "        forward_dict['exp']= part_1\n",
    "        forward_dict['sigmoid'] = part_3\n",
    "        forward_dict['tanh'] = part_2\n",
    "        forward_dict['loss'] = Loss\n",
    "        forward_dict['dy_pred'] = dy_pred\n",
    "        forward_dict['y_hat'] = y_hat\n",
    "        \n",
    "        return forward_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.14498615, 1.10584621, 1.14002452, 1.13582027, 1.11385062])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.ones(9)*0.1\n",
    "val_1= (w[0]*X[0]+w[1]*X[1]) * (w[0]*X[0]+w[1]*X[1]) + w[5]\n",
    "part_1 = np.exp(val_1)\n",
    "part_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.ones(9)*0.1\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vP0iV7f4fTKK",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_forwardprop(data):\n",
    "    dl = (data['dy_pred']==-1.9285278284819143)\n",
    "    loss=(data['loss']==0.9298048963072919)\n",
    "    part1=(data['exp']==1.1272967040973583)\n",
    "    part2=(data['tanh']==0.8417934192562146)\n",
    "    part3=(data['sigmoid']==0.5279179387419721)\n",
    "    assert(dl and loss and part1 and part2 and part3)\n",
    "    return True\n",
    "w=np.ones(9)*0.1\n",
    "d1=forward_propagation(X[0],y[0],w)\n",
    "grader_forwardprop(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YVYD18xfTKL"
   },
   "source": [
    "## <font color='blue'>Backward propagation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ld1PMQtffTKL"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(x,y,w,forward_dict):\n",
    "    '''In this function, we will compute the backward propagation '''\n",
    "    # forward_dict: the outputs of the forward_propagation() function\n",
    "    # write code to compute the gradients of each weight [w1,w2,w3,...,w9]\n",
    "    # Hint: you can use dict type to store the required variables \n",
    "    # dw1 = # in dw1 compute derivative of L w.r.to w1\n",
    "    # dw2 = # in dw2 compute derivative of L w.r.to w2\n",
    "    # dw3 = # in dw3 compute derivative of L w.r.to w3\n",
    "    # dw4 = # in dw4 compute derivative of L w.r.to w4\n",
    "    # dw5 = # in dw5 compute derivative of L w.r.to w5\n",
    "    # dw6 = # in dw6 compute derivative of L w.r.to w6\n",
    "    # dw7 = # in dw7 compute derivative of L w.r.to w7\n",
    "    # dw8 = # in dw8 compute derivative of L w.r.to w8\n",
    "    # dw9 = # in dw9 compute derivative of L w.r.to w9\n",
    "    \n",
    "    tnsqrt = forward_dict['tanh'] * forward_dict['tanh'] \n",
    "    sgmdvt = forward_dict['sigmoid'] * (1 - forward_dict['sigmoid'] )\n",
    "    \n",
    "    #val_1= (w[0]*x[0]+w[1]*x[1]) * (w[0]*x[0]+w[1]*x[1]) + w[5]\n",
    "    #part_1 = np.exp(val_1)\n",
    "    \n",
    "    dw1 = forward_dict['dy_pred'] * (1- tnsqrt) * forward_dict['exp'] * (w[0]*x[0]+w[1]*x[1]) * 2 * x[0]\n",
    "    \n",
    "    dw2 = forward_dict['dy_pred'] * (1- tnsqrt) * forward_dict['exp'] * (w[0]*x[0]+w[1]*x[1]) * 2 * x[1] \n",
    "    \n",
    "    dw3 = forward_dict['dy_pred'] * w[8] * sgmdvt * math.cos(w[2]*x[2]) * ((w[3] * x[3]) + (w[4] * x[4])) * x[2]\n",
    "    \n",
    "    dw4 = forward_dict['dy_pred'] * w[8] * sgmdvt * math.sin(w[2]*x[2]) * x[3]\n",
    "    \n",
    "    dw5 = forward_dict['dy_pred'] * w[8] * sgmdvt * math.sin(w[2]*x[2]) * x[4]\n",
    "    \n",
    "    dw6 = forward_dict['dy_pred'] *(1- tnsqrt) * forward_dict['exp']\n",
    "    \n",
    "    dw7 = forward_dict['dy_pred'] * (1 - tnsqrt)\n",
    "    \n",
    "    dw8 = forward_dict['dy_pred'] * w[8] * sgmdvt\n",
    "    \n",
    "    dw9 = forward_dict['dy_pred'] * forward_dict['sigmoid'] \n",
    "    backward_dict={}\n",
    "    #store the variables dw1,dw2 etc. in a dict as backward_dict['dw1']= dw1,backward_dict['dw2']= dw2...\n",
    "    \n",
    "    backward_dict['dw1']= dw1\n",
    "    backward_dict['dw2']= dw2\n",
    "    backward_dict['dw3']= dw3\n",
    "    backward_dict['dw4']= dw4\n",
    "    backward_dict['dw5']= dw5\n",
    "    backward_dict['dw6']= dw6\n",
    "    backward_dict['dw7']= dw7\n",
    "    backward_dict['dw8']= dw8\n",
    "    backward_dict['dw9']= dw9\n",
    "    \n",
    "    \n",
    "    return backward_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0ipQbNXOfTKM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_backprop(data):\n",
    "    dw1=(np.round(data['dw1'],6)==-0.229733)\n",
    "    dw2=(np.round(data['dw2'],6)==-0.021408)\n",
    "    dw3=(np.round(data['dw3'],6)==-0.005625)\n",
    "    dw4=(np.round(data['dw4'],6)==-0.004658)\n",
    "    dw5=(np.round(data['dw5'],6)==-0.001008)\n",
    "    dw6=(np.round(data['dw6'],6)==-0.633475)\n",
    "    dw7=(np.round(data['dw7'],6)==-0.561942)\n",
    "    dw8=(np.round(data['dw8'],6)==-0.048063)\n",
    "    dw9=(np.round(data['dw9'],6)==-1.018104)\n",
    "    assert(dw1 and dw2 and dw3 and dw4 and dw5 and dw6 and dw7 and dw8 and dw9)\n",
    "    return True \n",
    "w=np.ones(9)*0.1\n",
    "forward_dict=forward_propagation(X[0],y[0],w)\n",
    "backward_dict=backward_propagation(X[0],y[0],w,forward_dict)\n",
    "grader_backprop(backward_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.22973323498702"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_dict['dw1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STS4NrQQc6OH"
   },
   "source": [
    " ## <font color='blue'>Gradient clipping</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XY7ivRNzdPok"
   },
   "source": [
    "<b> Check this  <a href='https://towardsdatascience.com/how-to-debug-a-neural-network-with-gradient-checking-41deec0357a9'>blog link</a> for more details on Gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrsfpDoidtZ5"
   },
   "source": [
    " we know that the derivative of any function is\n",
    " \n",
    " $$\\lim_{\\epsilon\\to0}\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUcmt0kPd02f"
   },
   "source": [
    "*  The definition above can be used as a numerical approximation of the derivative. Taking an epsilon small enough, the calculated approximation will have an error in the range of epsilon squared. \n",
    "\n",
    "*  In other words, if epsilon is 0.001, the approximation will be off by 0.00001.\n",
    "\n",
    "Therefore, we can use this to approximate the gradient, and in turn make sure that backpropagation is implemented properly. This forms the basis of <b>gradient checking!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFSu16KCeU0x"
   },
   "source": [
    "## <Font color='blue'>Gradient checking example</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz0mmT_xecfC"
   },
   "source": [
    "<font >\n",
    "lets understand the concept with a simple example:\n",
    "$f(w1,w2,x1,x2)=w_{1}^{2} . x_{1} + w_{2} . x_{2}$ \n",
    "\n",
    "from the above function , lets assume $w_{1}=1$, $w_{2}=2$, $x_{1}=3$, $x_{2}=4$ the gradient of $f$ w.r.t $w_{1}$ is\n",
    "\n",
    "\\begin{array} {lcl}\n",
    "\\frac{df}{dw_{1}} = dw_{1} &=&2.w_{1}.x_{1} \\\\& = &2.1.3\\\\& = &6\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "let calculate the aproximate gradient of $w_{1}$ as mentinoned in the above formula and considering $\\epsilon=0.0001$\n",
    "\n",
    "\\begin{array} {lcl}\n",
    "dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((1+0.0001)^{2} . 3 + 2 . 4) - ((1-0.0001)^{2} . 3 + 2 . 4)}{2\\epsilon} \\\\ & = & \\frac{(1.00020001 . 3 + 2 . 4) - (0.99980001. 3 + 2 . 4)}{2*0.0001} \\\\ & = & \\frac{(11.00060003) - (10.99940003)}{0.0002}\\\\ & = & 5.99999999999\n",
    "\\end{array}\n",
    "\n",
    "Then, we apply the following formula for gradient check: <i>gradient_check</i> = \n",
    "$\\frac{\\left\\Vert\\left (dW-dW^{approx}\\rm\\right) \\right\\Vert_2}{\\left\\Vert\\left (dW\\rm\\right) \\right\\Vert_2+\\left\\Vert\\left (dW^{approx}\\rm\\right) \\right\\Vert_2}$\n",
    "\n",
    "The equation above is basically the Euclidean distance normalized by the sum of the norm of the vectors. We use normalization in case that one of the vectors is very small.\n",
    "As a value for epsilon, we usually opt for 1e-7. Therefore, if gradient check return a value less than 1e-7, then it means that backpropagation was implemented correctly. Otherwise, there is potentially a mistake in your implementation. If the value exceeds 1e-3, then you are sure that the code is not correct.\n",
    "\n",
    "in our example: <i>gradient_check</i> $ = \\frac{(6 - 5.999999999994898)}{(6 + 5.999999999994898)} = 4.2514140356330737e^{-13}$\n",
    "\n",
    "you can mathamatically derive the same thing like this\n",
    "\n",
    "\\begin{array} {lcl}\n",
    "dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((w_{1}+\\epsilon)^{2} . x_{1} + w_{2} . x_{2}) - ((w_{1}-\\epsilon)^{2} . x_{1} + w_{2} . x_{2})}{2\\epsilon} \\\\ & = & \\frac{4. \\epsilon.w_{1}. x_{1}}{2\\epsilon} \\\\ & = &  2.w_{1}.x_{1}\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1BFQQaCermK"
   },
   "source": [
    "## <font color='red'> Implement Gradient checking </font> <br>\n",
    " (Write your code in <font color='blue'> def gradient_checking()</font>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqpfA3AqfJba"
   },
   "source": [
    "**Algorithm** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL39KeRFfNoD"
   },
   "source": [
    "<pre>\n",
    "<font color='darkblue'>\n",
    "W = initilize_randomly\n",
    "def gradient_checking(data_point, W):<font color='grey'>\n",
    "    # compute the L value using forward_propagation()\n",
    "    # compute the gradients of W using backword_propagation()</font>\n",
    "    approx_gradients = []\n",
    "    for each wi weight value in W:<font color='grey'>\n",
    "        # add a small value to weight wi, and then find the values of L with the updated weights\n",
    "        # subtract a small value to weight wi, and then find the values of L with the updated weights\n",
    "        # compute the approximation gradients of weight wi</font>\n",
    "        approx_gradients.append(approximation gradients of weight wi)<font color='grey'>\n",
    "    # compare the gradient of weights W from backword_propagation() with the aproximation gradients of weights with <br>  gradient_check formula</font>\n",
    "    return gradient_check</font>\n",
    "<b>NOTE: you can do sanity check by checking all the return values of gradient_checking(),<br> they have to be zero. if not you have bug in your code\n",
    "</pre></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=[ 0.00271756,  0.01260512,  0.00167639, -0.00207756,  0.00720768,\n",
    "   0.00114524,  0.00684168,  0.02242521,  0.01296444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dw1': -0.011663051229887736,\n",
       " 'dw2': -0.0010868175306762123,\n",
       " 'dw3': 3.5520107457967047e-06,\n",
       " 'dw4': -1.1490564996805255e-05,\n",
       " 'dw5': -2.485927655890358e-06,\n",
       " 'dw6': -0.9032758006126373,\n",
       " 'dw7': -0.9022192541758162,\n",
       " 'dw8': -0.0070475910045734325,\n",
       " 'dw9': -1.0995465311667147}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_dict=forward_propagation(X[0],y[0],w)\n",
    "backward_dict=backward_propagation(X[0],y[0],w,forward_dict)\n",
    "backward_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.011663051229887736,\n",
       " -0.0010868175306762123,\n",
       " 3.5520107457967047e-06,\n",
       " -1.1490564996805255e-05,\n",
       " -2.485927655890358e-06,\n",
       " -0.9032758006126373,\n",
       " -0.9022192541758162,\n",
       " -0.0070475910045734325,\n",
       " -1.0995465311667147]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(backward_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XBoJRqAwfTKO"
   },
   "outputs": [],
   "source": [
    "def gradient_checking(x,y,w,eps):\n",
    "    # compute the dict value using forward_propagation()\n",
    "    # compute the actual gradients of W using backword_propagation()\n",
    "    forward_dict=forward_propagation(x,y,w)\n",
    "    backward_dict=backward_propagation(x,y,w,forward_dict)\n",
    "    \n",
    "    #we are storing the original gradients for the given datapoints in a list\n",
    "    \n",
    "    original_gradients_list=list(backward_dict.values())\n",
    "    # make sure that the order is correct i.e. first element in the list corresponds to  dw1 ,second element is dw2 etc.\n",
    "    # you can use reverse function if the values are in reverse order\n",
    "    \n",
    "    approx_gradients_list=[]\n",
    "    #now we have to write code for approx gradients, here you have to make sure that you update only one weight at a time\n",
    "    #write your code here and append the approximate gradient value for each weight in  approx_gradients_list\n",
    "    \n",
    "    for i in range(len(w)):\n",
    "        currnt = w[i]\n",
    "        w[i] = currnt + eps\n",
    "        forward_dict=forward_propagation(x,y,w)\n",
    "        L1 = forward_dict['loss']\n",
    "        w[i] = currnt - eps\n",
    "        forward_dict=forward_propagation(x,y,w)\n",
    "        L2 = forward_dict['loss']\n",
    "        \n",
    "        approx_grad_wi = (L1 - L2) / (2* eps)\n",
    "        approx_gradients_list.append(approx_grad_wi)\n",
    "        \n",
    "        \n",
    "    #performing gradient check operation\n",
    "    original_gradients_list=np.array(original_gradients_list)\n",
    "    approx_gradients_list=np.array(approx_gradients_list)\n",
    "    gradient_check_value =(original_gradients_list-approx_gradients_list)/(original_gradients_list+approx_gradients_list)\n",
    "    \n",
    "    return gradient_check_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aWSU56GffTKO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.73921918e-08  1.28741906e-05 -2.55164399e-04 -1.05871856e-05\n",
      " -1.95446016e-04 -1.16536595e-10 -9.63625495e-08 -1.06774472e-07\n",
      " -1.43339489e-08]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_grad_check(value):\n",
    "    print(value)\n",
    "    assert(np.all(value <= 10**-3))\n",
    "    return True \n",
    "\n",
    "w=[ 0.00271756,  0.01260512,  0.00167639, -0.00207756,  0.00720768,\n",
    "   0.00114524,  0.00684168,  0.02242521,  0.01296444]\n",
    "\n",
    "eps=10**-7\n",
    "value= gradient_checking(X[0],y[0],w,eps)\n",
    "grader_grad_check(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zt05soYh1RM"
   },
   "source": [
    "* As a part of this task, you will be implementing 2  optimizers(methods to update weight)\n",
    "* Use the same computational graph that was mentioned above to do this task\n",
    "* The weights have been initialized from normal distribution with mean=0 and std=0.01. The initialization of weights is very important otherwiswe you can face vanishing gradient and exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAyi7aSAicbr"
   },
   "source": [
    "**Check below video for reference purpose**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmefh7ktjbaR"
   },
   "source": [
    "<font color='blue'><b>Algorithm</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAc4NudkjdNa"
   },
   "source": [
    "<pre>\n",
    "    for each epoch(1-20):\n",
    "        for each data point in your data:\n",
    "            using the functions forward_propagation() and backword_propagation() compute the gradients of weights\n",
    "            update the weigts with help of gradients  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fdmPNqtjm3X"
   },
   "source": [
    "## <font color='red'> Implement below tasks</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll9-CRsLjx_D"
   },
   "source": [
    "\n",
    "*  <b>Task 2.1</b>: you will be implementing the above algorithm with <b>Vanilla update</b> of weights<br><br>\n",
    "*  <b>Task 2.2</b>: you will be implementing the above algorithm with <b>Momentum update</b> of weights<br><br>\n",
    "*  <b>Task 2.3</b>: you will be implementing the above algorithm with <b>Adam update</b> of weights<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atp082demrUR"
   },
   "source": [
    "**Note : If you get any assertion error while running grader functions, please print the variables in grader functions and check which variable is returning False .Recheck your logic for that variable .**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cv_hZFWalS2z"
   },
   "source": [
    "### <font color='blue'>2.1 Algorithm with Vanilla update of weights</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KlVuEsRjQoW5"
   },
   "outputs": [],
   "source": [
    "train_loss_v = []\n",
    "def vanilaUpdate(X_train,eta0,epochs,w):\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        inter_loss = []\n",
    "        for i in range(len(X_train)):\n",
    "            forward_dict=forward_propagation(X_train[i],y[i],w)\n",
    "            \n",
    "            backward_dict=backward_propagation(X_train[i],y[i],w,forward_dict)\n",
    "            dw = np.array(list(backward_dict.values()))\n",
    "            w = w - (eta0 * dw)\n",
    "            inter_loss.append(forward_dict['loss'])\n",
    "\n",
    "        train_loss_v.append(np.mean(inter_loss))\n",
    "        print(\"epoch No :\",j,end=\"\\t\")\n",
    "        print(\"Loss :\",np.mean(inter_loss))\n",
    " \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "HuDaxRnCQoW5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00271746, 0.01260502, 0.00167629, -0.00207766, 0.00720758, 0.00114514, 0.00684158, 0.02242511, 0.012964340000000001]\n",
      "epoch No : 0\tLoss : 0.9906877485343311\n",
      "epoch No : 1\tLoss : 0.8914692674599634\n",
      "epoch No : 2\tLoss : 0.8099594440068556\n",
      "epoch No : 3\tLoss : 0.7412259470544349\n",
      "epoch No : 4\tLoss : 0.6820491632145764\n",
      "epoch No : 5\tLoss : 0.6302463684715147\n",
      "epoch No : 6\tLoss : 0.5842884820896366\n",
      "epoch No : 7\tLoss : 0.5430733921767518\n",
      "epoch No : 8\tLoss : 0.5057867081766028\n",
      "epoch No : 9\tLoss : 0.47181327690119784\n",
      "epoch No : 10\tLoss : 0.4406792358082338\n",
      "epoch No : 11\tLoss : 0.4120130316946212\n",
      "epoch No : 12\tLoss : 0.38551856169027743\n",
      "epoch No : 13\tLoss : 0.3609562666347231\n",
      "epoch No : 14\tLoss : 0.3381295658113445\n",
      "epoch No : 15\tLoss : 0.31687495719014547\n",
      "epoch No : 16\tLoss : 0.2970546829913006\n",
      "epoch No : 17\tLoss : 0.2785512231717963\n",
      "epoch No : 18\tLoss : 0.2612631130438977\n",
      "epoch No : 19\tLoss : 0.24510173465636134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01907502,  0.03149075,  0.00195998, -0.00194013,  0.00731665,\n",
       "        0.35801917,  0.30641089,  0.14602163,  0.71867958])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta0=0.0001\n",
    "epochs=20\n",
    "print(w)\n",
    "vanilaUpdate(X,eta0,epochs,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4RWWrZ7lWap"
   },
   "source": [
    "### <font color='blue'>2.2 Algorithm with Momentum update of weights</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM2BAasL6zz4"
   },
   "source": [
    "<img src='https://i.imgur.com/gyPSXhS.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iCc4sIE-4A-"
   },
   "source": [
    "Here Gamma referes to the momentum coefficient, eta is leaning rate and v_t is moving average of our gradients at timestep t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KTmDomaQoW5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_m = []\n",
    "def momentumUpdate(X_train,eta0,epochs,w,gamma):\n",
    "    v_prev = 0\n",
    "    for j in range(epochs):\n",
    "        inter_loss = []\n",
    "        for i in range(len(X_train)):\n",
    "            forward_dict=forward_propagation(X_train[i],y[i],w)\n",
    "            backward_dict=backward_propagation(X_train[i],y[i],w,forward_dict)\n",
    "            dw = np.array(list(backward_dict.values()))\n",
    "            \n",
    "            v = (gamma * v_prev) + (eta0 * dw)\n",
    "            \n",
    "            w = w - v\n",
    "            \n",
    "            v_prev = v\n",
    "        \n",
    "            inter_loss.append(forward_dict['loss'])\n",
    "\n",
    "        train_loss_m.append(np.mean(inter_loss))\n",
    "        print(\"epoch No :\",j,end=\"\\t\")\n",
    "        print(\"Loss :\",np.mean(inter_loss))\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00271746, 0.01260502, 0.00167629, -0.00207766, 0.00720758, 0.00114514, 0.00684158, 0.02242511, 0.012964340000000001]\n",
      "epoch No : 0\tLoss : 0.6907689464638022\n",
      "epoch No : 1\tLoss : 0.33189409869585795\n",
      "epoch No : 2\tLoss : 0.1733873453218329\n",
      "epoch No : 3\tLoss : 0.09430741504828742\n",
      "epoch No : 4\tLoss : 0.05713614406344189\n",
      "epoch No : 5\tLoss : 0.04106294607186752\n",
      "epoch No : 6\tLoss : 0.03464551332969455\n",
      "epoch No : 7\tLoss : 0.032279883251263734\n",
      "epoch No : 8\tLoss : 0.03149309303965892\n",
      "epoch No : 9\tLoss : 0.03127790550858522\n",
      "epoch No : 10\tLoss : 0.03125013361913477\n",
      "epoch No : 11\tLoss : 0.031272732748596356\n",
      "epoch No : 12\tLoss : 0.03130125748353921\n",
      "epoch No : 13\tLoss : 0.03132388839986447\n",
      "epoch No : 14\tLoss : 0.03133928085215153\n",
      "epoch No : 15\tLoss : 0.031348837027597345\n",
      "epoch No : 16\tLoss : 0.03135424514384684\n",
      "epoch No : 17\tLoss : 0.03135684139329974\n",
      "epoch No : 18\tLoss : 0.03135754793511056\n",
      "epoch No : 19\tLoss : 0.031356958324924196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.03772262, 0.04948852, 0.01131505, 0.00146758, 0.01337534,\n",
       "       0.50651104, 0.40387316, 0.42767226, 1.35914326])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta0=0.0001\n",
    "epochs=20\n",
    "g = 0.9\n",
    "print(w)\n",
    "momentumUpdate(X,eta0,epochs,w,g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Agtmr_lZQoW6"
   },
   "source": [
    "### <font color='blue'>2.3 Algorithm with Adam update of weights</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRYiT2KKQoW6"
   },
   "source": [
    "<img src='https://i.imgur.com/GDR8UFP.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0YffVQ2LQoW6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_loss_a = []\n",
    "def AdamUpdate(X_train,eta0,epochs,w,beta_1,beta_2,eps):\n",
    "    m_prev = 0\n",
    "    v_prev = 0\n",
    "    for t in range(epochs):\n",
    "        inter_loss = []\n",
    "        for i in range(len(X_train)):\n",
    "            forward_dict=forward_propagation(X_train[i],y[i],w)\n",
    "            backward_dict=backward_propagation(X_train[i],y[i],w,forward_dict)\n",
    "            dw = np.array(list(backward_dict.values()))\n",
    "            \n",
    "            m = (beta_1 * m_prev) + ((1 - beta_1) * dw)\n",
    "            \n",
    "            v = (beta_2 * v_prev) + ((1 - beta_2) * (dw**2))\n",
    "            \n",
    "            m_hat = m/(1 - (beta_1**(t+1)) )\n",
    "            v_hat = v/(1 - (beta_2**(t+1)) )\n",
    "            \n",
    "            sr = np.sqrt(v_hat)\n",
    "            w = w - (eta0/sr + eps) * m_hat\n",
    "            \n",
    "            m_prev = m\n",
    "            v_prev = v\n",
    "        \n",
    "            inter_loss.append(forward_dict['loss'])\n",
    "\n",
    "        train_loss_a.append(np.mean(inter_loss))\n",
    "        print(\"epoch No :\",t,end=\"\\t\")\n",
    "        print(\"Loss :\",np.mean(inter_loss))\n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00271746, 0.01260502, 0.00167629, -0.00207766, 0.00720758, 0.00114514, 0.00684158, 0.02242511, 0.012964340000000001]\n",
      "epoch No : 0\tLoss : 0.5382304003909644\n",
      "epoch No : 1\tLoss : 0.2556716348231495\n",
      "epoch No : 2\tLoss : 0.1583133965157599\n",
      "epoch No : 3\tLoss : 0.10134489319161617\n",
      "epoch No : 4\tLoss : 0.06450700553866746\n",
      "epoch No : 5\tLoss : 0.04006103600139304\n",
      "epoch No : 6\tLoss : 0.024031546723058555\n",
      "epoch No : 7\tLoss : 0.013933255330501637\n",
      "epoch No : 8\tLoss : 0.007965930596304995\n",
      "epoch No : 9\tLoss : 0.00471168811102941\n",
      "epoch No : 10\tLoss : 0.0030576927147607717\n",
      "epoch No : 11\tLoss : 0.0022135564428607973\n",
      "epoch No : 12\tLoss : 0.0017157343960657171\n",
      "epoch No : 13\tLoss : 0.0013576875305438812\n",
      "epoch No : 14\tLoss : 0.0010746724449321823\n",
      "epoch No : 15\tLoss : 0.0008514139142218007\n",
      "epoch No : 16\tLoss : 0.0006796881892599136\n",
      "epoch No : 17\tLoss : 0.0005489545090615768\n",
      "epoch No : 18\tLoss : 0.00044807567723667324\n",
      "epoch No : 19\tLoss : 0.0003677858924152262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.47758597,  0.45666917, -0.99420947, -1.0375236 , -0.82816739,\n",
       "        0.54139033,  0.45277442,  1.01560472,  1.00683094])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta0=0.001\n",
    "epochs=20\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "eps=10**-8\n",
    "print(w)\n",
    "AdamUpdate(X,eta0,epochs,w,beta_1,beta_2,eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7627cy8YlnYO"
   },
   "source": [
    "<font color='blue'>Comparision plot between epochs and loss with different optimizers. Make sure that loss is conerging with increaing epochs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "scleSfIXl_bC"
   },
   "outputs": [],
   "source": [
    "#plot the graph between loss vs epochs for all 3 optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch = [ i for i in range(epochs) ]\n",
    "\n",
    "figure,ax = plt.subplots()\n",
    "ax.plot(epoch,train_loss_v,label = \"Vanilla\")\n",
    "ax.plot(epoch,train_loss_m,label = \"Momentum\")\n",
    "ax.plot(epoch,train_loss_a,label = \"Adam\")\n",
    "ax.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfDWEOCr69uQ"
   },
   "source": [
    "<font color='blue'><b>You can go through the following blog to understand the implementation of other optimizers .</font>\n",
    "   <br> [Gradients update blog](https://cs231n.github.io/neural-networks-3/) </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsoSyrCQ6_xb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Backpropagation_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
