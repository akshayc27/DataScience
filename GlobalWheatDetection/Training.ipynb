{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqErMtkdvWYt",
    "papermill": {
     "duration": 0.048939,
     "end_time": "2022-03-20T16:50:07.882813",
     "exception": false,
     "start_time": "2022-03-20T16:50:07.833874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading all the necesssary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:07.996130Z",
     "iopub.status.busy": "2022-03-20T16:50:07.995318Z",
     "iopub.status.idle": "2022-03-20T16:50:14.579969Z",
     "shell.execute_reply": "2022-03-20T16:50:14.579384Z",
     "shell.execute_reply.started": "2022-03-20T11:14:12.883826Z"
    },
    "id": "UVq2ukj-u5wj",
    "outputId": "13c3f7a7-8ddb-458b-8f45-8566a942021f",
    "papermill": {
     "duration": 6.647463,
     "end_time": "2022-03-20T16:50:14.580122",
     "exception": false,
     "start_time": "2022-03-20T16:50:07.932659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Add,Concatenate,Conv2D,Input,Lambda,LeakyReLU,MaxPool2D,UpSampling2D,ZeroPadding2D,BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import binary_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,TensorBoard\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, LearningRateScheduler, CSVLogger\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from seaborn import color_palette\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTSiTM_8OrO_",
    "papermill": {
     "duration": 0.048411,
     "end_time": "2022-03-20T16:50:14.677712",
     "exception": false,
     "start_time": "2022-03-20T16:50:14.629301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:14.781113Z",
     "iopub.status.busy": "2022-03-20T16:50:14.780332Z",
     "iopub.status.idle": "2022-03-20T16:50:14.786060Z",
     "shell.execute_reply": "2022-03-20T16:50:14.785537Z",
     "shell.execute_reply.started": "2022-03-20T11:14:12.898337Z"
    },
    "papermill": {
     "duration": 0.060112,
     "end_time": "2022-03-20T16:50:14.786195",
     "exception": false,
     "start_time": "2022-03-20T16:50:14.726083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/global-wheat-detection\n"
     ]
    }
   ],
   "source": [
    "cd ../input/global-wheat-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:14.889336Z",
     "iopub.status.busy": "2022-03-20T16:50:14.888725Z",
     "iopub.status.idle": "2022-03-20T16:50:15.158267Z",
     "shell.execute_reply": "2022-03-20T16:50:15.157762Z",
     "shell.execute_reply.started": "2022-03-20T11:14:12.909127Z"
    },
    "id": "DbZPU1_gOxPb",
    "papermill": {
     "duration": 0.322942,
     "end_time": "2022-03-20T16:50:15.158408",
     "exception": false,
     "start_time": "2022-03-20T16:50:14.835466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv') #Load the CSV file\n",
    "train_df['image_id'] = 'train/' + train_df['image_id'].astype(str)+'.jpg' #Add the path to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:15.262630Z",
     "iopub.status.busy": "2022-03-20T16:50:15.261931Z",
     "iopub.status.idle": "2022-03-20T16:50:15.273339Z",
     "shell.execute_reply": "2022-03-20T16:50:15.273747Z",
     "shell.execute_reply.started": "2022-03-20T11:14:13.106338Z"
    },
    "papermill": {
     "duration": 0.066641,
     "end_time": "2022-03-20T16:50:15.273887",
     "exception": false,
     "start_time": "2022-03-20T16:50:15.207246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>bbox</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 222.0, 56.0, 36.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[226.0, 548.0, 130.0, 58.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[377.0, 504.0, 74.0, 160.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[834.0, 95.0, 109.0, 107.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>[26.0, 144.0, 124.0, 117.0]</td>\n",
       "      <td>usask_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_id  width  height                         bbox   source\n",
       "0  train/b6ab77fd7.jpg   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n",
       "1  train/b6ab77fd7.jpg   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n",
       "2  train/b6ab77fd7.jpg   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n",
       "3  train/b6ab77fd7.jpg   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n",
       "4  train/b6ab77fd7.jpg   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_AXWkDGO0Ft",
    "papermill": {
     "duration": 0.048675,
     "end_time": "2022-03-20T16:50:15.371640",
     "exception": false,
     "start_time": "2022-03-20T16:50:15.322965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:15.484559Z",
     "iopub.status.busy": "2022-03-20T16:50:15.480249Z",
     "iopub.status.idle": "2022-03-20T16:50:17.455716Z",
     "shell.execute_reply": "2022-03-20T16:50:17.456132Z",
     "shell.execute_reply.started": "2022-03-20T11:14:13.119896Z"
    },
    "id": "zURqRyzuxPCo",
    "papermill": {
     "duration": 2.035492,
     "end_time": "2022-03-20T16:50:17.456331",
     "exception": false,
     "start_time": "2022-03-20T16:50:15.420839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[['x_min','y_min', 'width', 'height']] = pd.DataFrame([ast.literal_eval(x) for x in train_df.bbox.tolist()], index= train_df.index)\n",
    "train_df = train_df[['image_id', 'bbox', 'source', 'x_min', 'y_min', 'width', 'height']]\n",
    "train_df['area'] = train_df['width'] * train_df['height']\n",
    "train_df['x_max'] = train_df['x_min'] + train_df['width']\n",
    "train_df['y_max'] = train_df['y_min'] + train_df['height']\n",
    "train_df = train_df.drop(['bbox', 'source'], axis=1)\n",
    "train_df = train_df[['image_id', 'x_min', 'y_min', 'x_max', 'y_max', 'width', 'height', 'area']]\n",
    "\n",
    "# There are some buggy annonations in training images having huge bounding boxes. Let's remove those bboxes\n",
    "train_df = train_df[train_df['area'] < 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:17.569852Z",
     "iopub.status.busy": "2022-03-20T16:50:17.569145Z",
     "iopub.status.idle": "2022-03-20T16:50:17.572146Z",
     "shell.execute_reply": "2022-03-20T16:50:17.572780Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.567424Z"
    },
    "papermill": {
     "duration": 0.066748,
     "end_time": "2022-03-20T16:50:17.572978",
     "exception": false,
     "start_time": "2022-03-20T16:50:17.506230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>7540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>11840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>11663.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>14508.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image_id  x_min  y_min  x_max  y_max  width  height     area\n",
       "0  train/b6ab77fd7.jpg  834.0  222.0  890.0  258.0   56.0    36.0   2016.0\n",
       "1  train/b6ab77fd7.jpg  226.0  548.0  356.0  606.0  130.0    58.0   7540.0\n",
       "2  train/b6ab77fd7.jpg  377.0  504.0  451.0  664.0   74.0   160.0  11840.0\n",
       "3  train/b6ab77fd7.jpg  834.0   95.0  943.0  202.0  109.0   107.0  11663.0\n",
       "4  train/b6ab77fd7.jpg   26.0  144.0  150.0  261.0  124.0   117.0  14508.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:17.683616Z",
     "iopub.status.busy": "2022-03-20T16:50:17.682982Z",
     "iopub.status.idle": "2022-03-20T16:50:17.685668Z",
     "shell.execute_reply": "2022-03-20T16:50:17.686065Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.583999Z"
    },
    "papermill": {
     "duration": 0.057278,
     "end_time": "2022-03-20T16:50:17.686209",
     "exception": false,
     "start_time": "2022-03-20T16:50:17.628931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147769, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:17.803572Z",
     "iopub.status.busy": "2022-03-20T16:50:17.802708Z",
     "iopub.status.idle": "2022-03-20T16:50:17.874282Z",
     "shell.execute_reply": "2022-03-20T16:50:17.873823Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.592823Z"
    },
    "id": "X3RNlcZFO7MT",
    "outputId": "af5c18a9-f656-49df-f10f-0a65c0dbae20",
    "papermill": {
     "duration": 0.138448,
     "end_time": "2022-03-20T16:50:17.874415",
     "exception": false,
     "start_time": "2022-03-20T16:50:17.735967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_h</th>\n",
       "      <th>max_w</th>\n",
       "      <th>min_h</th>\n",
       "      <th>min_w</th>\n",
       "      <th>category</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>606.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202.0</td>\n",
       "      <td>943.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>train/b6ab77fd7.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_h  max_w  min_h  min_w category             img_path  label\n",
       "0  258.0  890.0  222.0  834.0    Wheat  train/b6ab77fd7.jpg      1\n",
       "1  606.0  356.0  548.0  226.0    Wheat  train/b6ab77fd7.jpg      1\n",
       "2  664.0  451.0  504.0  377.0    Wheat  train/b6ab77fd7.jpg      1\n",
       "3  202.0  943.0   95.0  834.0    Wheat  train/b6ab77fd7.jpg      1\n",
       "4  261.0  150.0  144.0   26.0    Wheat  train/b6ab77fd7.jpg      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(['width','height','area'],axis = 1) #Dropping unwanted columns\n",
    "class_list = ['Wheat']*train_df.shape[0]  \n",
    "train_df['category'] = class_list  #Adding category column\n",
    "label_list = [1]*train_df.shape[0] \n",
    "train_df['label'] = label_list #Adding class label to each row\n",
    "train_df.columns = ['img_path','min_w','min_h','max_w','max_h','category','label'] #Changing Column names\n",
    "\n",
    "df = train_df  #Loading a copy into df\n",
    "df = df[['max_h', 'max_w', 'min_h', 'min_w', 'category', 'img_path', 'label']] #Changing the order of columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnO3WkhEPFTf",
    "papermill": {
     "duration": 0.052542,
     "end_time": "2022-03-20T16:50:17.977826",
     "exception": false,
     "start_time": "2022-03-20T16:50:17.925284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:18.106827Z",
     "iopub.status.busy": "2022-03-20T16:50:18.100448Z",
     "iopub.status.idle": "2022-03-20T16:50:18.187607Z",
     "shell.execute_reply": "2022-03-20T16:50:18.187121Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.684968Z"
    },
    "id": "bEtQGSrkxR6Y",
    "outputId": "fccf9544-9440-4b89-dba7-91a2d730143c",
    "papermill": {
     "duration": 0.155607,
     "end_time": "2022-03-20T16:50:18.187733",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.032126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of boxes in a given image are : 116\n",
      "Total number of unique images in the dataframe : 3373\n",
      "Splitting into train and test data......\n",
      "Number of images in train data : 2500\n",
      "Number of images in test data  : 873\n"
     ]
    }
   ],
   "source": [
    "max_boxes = df.groupby(['img_path']).count()['category'].max()\n",
    "print('Max number of boxes in a given image are :',max_boxes) #Calculating maximum number of boxes in an image\n",
    "\n",
    "index = list(set([i.split('/')[-1] for i in df['img_path'].values]))\n",
    "print('Total number of unique images in the dataframe :',len(index))  #Taking out all the unique images in the df\n",
    "\n",
    "#Here for simplicity sake i am using 1000 images as train and 200 images as test.\n",
    "print('Splitting into train and test data......')\n",
    "train_image=index[0:2500]\n",
    "test_image =index[2500:3733]\n",
    "print('Number of images in train data :',len(train_image))\n",
    "print('Number of images in test data  :',len(test_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLc3w9e4o5Qa",
    "papermill": {
     "duration": 0.050068,
     "end_time": "2022-03-20T16:50:18.288900",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.238832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Initializing all the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:18.398534Z",
     "iopub.status.busy": "2022-03-20T16:50:18.397453Z",
     "iopub.status.idle": "2022-03-20T16:50:18.399732Z",
     "shell.execute_reply": "2022-03-20T16:50:18.400318Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.784196Z"
    },
    "id": "5_RE30UXo2Zi",
    "papermill": {
     "duration": 0.060098,
     "end_time": "2022-03-20T16:50:18.400494",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.340396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yolo_anchors = np.array([(10, 13), (16, 30), (33, 23), (30, 61), (62, 45),\n",
    "                         (59, 119), (116, 90), (156, 198), (373, 326)],\n",
    "                        np.float32) / 320\n",
    "yolo_anchor_masks = np.array([[6, 7, 8], [3, 4, 5], [0, 1, 2]])\n",
    "\n",
    "size = 320 # size of resize image\n",
    "batch_size = 8 #Batch size that we want to load when training\n",
    "yolo_max_boxes = 116 # maximum yolo boxes predicted per image.\n",
    "yolo_iou_threshold = 0.5 # IOU threshold score \n",
    "yolo_score_threshold = 0.4 # objectness threshold score\n",
    "learning_rate = 1e-4 # learning rate\n",
    "num_classes = 2 # num of category in our dataset\n",
    "epochs = 30 # epochs run to fine tune our model\n",
    "class_dict = {'Wheat':1} # Mapping of category and its corresponding label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiogsVmOpJFf",
    "papermill": {
     "duration": 0.050726,
     "end_time": "2022-03-20T16:50:18.502631",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.451905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# YOLOV3 architecture starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:18.610542Z",
     "iopub.status.busy": "2022-03-20T16:50:18.609769Z",
     "iopub.status.idle": "2022-03-20T16:50:18.612252Z",
     "shell.execute_reply": "2022-03-20T16:50:18.611793Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.793386Z"
    },
    "id": "oDBKh_S-o2n1",
    "papermill": {
     "duration": 0.059393,
     "end_time": "2022-03-20T16:50:18.612368",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.552975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DarknetConv(x, filters, size, strides=1, batch_norm=True):\n",
    "    if strides == 1: \n",
    "        padding = 'same' \n",
    "    else:\n",
    "        x = ZeroPadding2D(((1, 0), (1, 0)))(x)  # top left half-padding \n",
    "        padding = 'valid' \n",
    "    x = Conv2D(filters=filters, kernel_size=size,\n",
    "               strides=strides, padding=padding,\n",
    "               use_bias=not batch_norm, kernel_regularizer=l2(0.0005))(x)\n",
    "    if batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x) # by default alpha is 0.3\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:18.719312Z",
     "iopub.status.busy": "2022-03-20T16:50:18.718488Z",
     "iopub.status.idle": "2022-03-20T16:50:18.720959Z",
     "shell.execute_reply": "2022-03-20T16:50:18.720547Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.802631Z"
    },
    "id": "Jy9xkN0WQn7j",
    "papermill": {
     "duration": 0.057591,
     "end_time": "2022-03-20T16:50:18.721072",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.663481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# below is residual connection layer function\n",
    "def DarknetResidual(x, filters):\n",
    "    prev = x # storing input in prev variable \n",
    "    x = DarknetConv(x, filters // 2, 1) \n",
    "    x = DarknetConv(x, filters, 3)\n",
    "    x = Add()([prev, x]) # residual connection\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:18.829325Z",
     "iopub.status.busy": "2022-03-20T16:50:18.828445Z",
     "iopub.status.idle": "2022-03-20T16:50:18.830269Z",
     "shell.execute_reply": "2022-03-20T16:50:18.830721Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.811333Z"
    },
    "id": "BtfFrgujQp3u",
    "papermill": {
     "duration": 0.059365,
     "end_time": "2022-03-20T16:50:18.830858",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.771493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is our real Darknetblock function calling above 2 fucntions \n",
    "def DarknetBlock(x, filters, blocks):\n",
    "    x = DarknetConv(x, filters, 3, strides=2) \n",
    "    for _ in range(blocks):\n",
    "        x = DarknetResidual(x, filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:18.938303Z",
     "iopub.status.busy": "2022-03-20T16:50:18.937469Z",
     "iopub.status.idle": "2022-03-20T16:50:18.939936Z",
     "shell.execute_reply": "2022-03-20T16:50:18.939502Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.823684Z"
    },
    "id": "tLpcCT7mQr7g",
    "papermill": {
     "duration": 0.058317,
     "end_time": "2022-03-20T16:50:18.940043",
     "exception": false,
     "start_time": "2022-03-20T16:50:18.881726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Darknet(name=None):\n",
    "    x = inputs = Input([None, None, 3])\n",
    "    x = DarknetConv(x, 32, 3)\n",
    "    x = DarknetBlock(x, 64, 1)\n",
    "    x = DarknetBlock(x, 128, 2)  # skip connection\n",
    "    x = x_36 = DarknetBlock(x, 256, 8)  # skip connection\n",
    "    x = x_61 = DarknetBlock(x, 512, 8)\n",
    "    x = DarknetBlock(x, 1024, 4) # last layer detecting bounding box dimension (tx,ty,bx,by)\n",
    "    return tf.keras.Model(inputs, (x_36, x_61, x), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:19.073808Z",
     "iopub.status.busy": "2022-03-20T16:50:19.072831Z",
     "iopub.status.idle": "2022-03-20T16:50:19.075336Z",
     "shell.execute_reply": "2022-03-20T16:50:19.075763Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.83294Z"
    },
    "id": "kJEMjhLbo2kw",
    "papermill": {
     "duration": 0.075069,
     "end_time": "2022-03-20T16:50:19.075910",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.000841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def YoloConv(filters, name=None):\n",
    "    def yolo_conv(x_in):\n",
    "        if isinstance(x_in, tuple):\n",
    "            inputs = Input(x_in[0].shape[1:]), Input(x_in[1].shape[1:])\n",
    "            x, x_skip = inputs\n",
    "\n",
    "            # concat with skip connection\n",
    "            x = DarknetConv(x, filters, 1)\n",
    "            # upsampling of a layer\n",
    "            x = UpSampling2D(2)(x)\n",
    "            # concatenation of skip connection result and last output result\n",
    "            x = Concatenate()([x, x_skip])\n",
    "        else:\n",
    "            x = inputs = Input(x_in.shape[1:])\n",
    "        \n",
    "        x = DarknetConv(x, filters, 1)\n",
    "        x = DarknetConv(x, filters * 2, 3)\n",
    "        x = DarknetConv(x, filters, 1)\n",
    "        x = DarknetConv(x, filters * 2, 3)\n",
    "        x = DarknetConv(x, filters, 1)\n",
    "        return Model(inputs, x, name=name)(x_in)\n",
    "    return yolo_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTzcDo4TQ1AM",
    "papermill": {
     "duration": 0.057372,
     "end_time": "2022-03-20T16:50:19.194441",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.137069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## YOLO output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:19.314839Z",
     "iopub.status.busy": "2022-03-20T16:50:19.314003Z",
     "iopub.status.idle": "2022-03-20T16:50:19.316535Z",
     "shell.execute_reply": "2022-03-20T16:50:19.316105Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.842857Z"
    },
    "id": "ZTWKeWT8Q25a",
    "papermill": {
     "duration": 0.064027,
     "end_time": "2022-03-20T16:50:19.316650",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.252623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def YoloOutput(filters, anchors, classes, name=None):\n",
    "    def yolo_output(x_in):\n",
    "        x = inputs = Input(x_in.shape[1:]) # this is an input shape excluded with batch size.\n",
    "        x = DarknetConv(x, filters * 2, 3) # Darkconv is a fn implemented above which is internally calling\n",
    "        x = DarknetConv(x, anchors * (classes + 5), 1, batch_norm=False)\n",
    "        x = Lambda(lambda x: tf.reshape(x, (-1, tf.shape(x)[1], tf.shape(x)[2],\n",
    "                                            anchors, classes + 5)))(x)\n",
    "        # x is reshaped into (None, grid_size, grid_size, anchors, (x,y,w,h,objectness score,..classes))\n",
    "        return tf.keras.Model(inputs, x, name=name)(x_in)\n",
    "    return yolo_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2F0_pnVnQ-a1",
    "papermill": {
     "duration": 0.053114,
     "end_time": "2022-03-20T16:50:19.423951",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.370837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Non Max Supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:19.543613Z",
     "iopub.status.busy": "2022-03-20T16:50:19.542692Z",
     "iopub.status.idle": "2022-03-20T16:50:19.544526Z",
     "shell.execute_reply": "2022-03-20T16:50:19.545055Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.852401Z"
    },
    "id": "KFPMAJ4YRAeY",
    "papermill": {
     "duration": 0.069849,
     "end_time": "2022-03-20T16:50:19.545213",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.475364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def yolo_boxes(pred, anchors, classes):\n",
    "    # pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...classes))\n",
    "    grid_size = tf.shape(pred)[1]\n",
    "    box_xy, box_wh, objectness, class_probs = tf.split(\n",
    "        pred, (2, 2, 1, classes), axis=-1)\n",
    "    # objectness : it's means whether there is any object in a predicted box\n",
    "    # class_probs : it's a probability of a class given object is there i.e P(Pc|object)\n",
    "\n",
    "    box_xy = tf.sigmoid(box_xy) \n",
    "    objectness = tf.sigmoid(objectness)\n",
    "    class_probs = tf.sigmoid(class_probs)\n",
    "    pred_box = tf.concat((box_xy, box_wh), axis=-1)  # original xywh for loss\n",
    "\n",
    "    # !!! grid[x][y] == (y, x)\n",
    "    grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
    "    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n",
    "    box_xy = (box_xy + tf.cast(grid, tf.float32)) / \\\n",
    "        tf.cast(grid_size, tf.float32)\n",
    "    box_wh = tf.exp(box_wh) * anchors\n",
    "\n",
    "    box_x1y1 = box_xy - box_wh / 2 \n",
    "    box_x2y2 = box_xy + box_wh / 2\n",
    "    bbox = tf.concat([box_x1y1, box_x2y2], axis=-1)\n",
    "\n",
    "    return bbox, objectness, class_probs, pred_box\n",
    "\n",
    "def yolo_nms(outputs, anchors, masks, classes):\n",
    "    # boxes, confidence scores(objectness scores), class probabilities\n",
    "\n",
    "    b, c, t = [], [], []\n",
    "\n",
    "    # iterating through each outputs predicted by model\n",
    "    for o in outputs:\n",
    "        b.append(tf.reshape(o[0], (tf.shape(o[0])[0], -1, tf.shape(o[0])[-1])))\n",
    "        c.append(tf.reshape(o[1], (tf.shape(o[1])[0], -1, tf.shape(o[1])[-1])))\n",
    "        t.append(tf.reshape(o[2], (tf.shape(o[2])[0], -1, tf.shape(o[2])[-1])))\n",
    "\n",
    "    bbox = tf.concat(b, axis=1)\n",
    "    confidence = tf.concat(c, axis=1)\n",
    "    class_probs = tf.concat(t, axis=1)\n",
    "\n",
    "    scores = confidence * class_probs # this is P(Pc|objectness score) value\n",
    "    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\n",
    "        boxes=tf.reshape(bbox, (tf.shape(bbox)[0], -1, 1, 4)),\n",
    "        scores=tf.reshape(\n",
    "            scores, (tf.shape(scores)[0], -1, tf.shape(scores)[-1])),\n",
    "        max_output_size_per_class = yolo_max_boxes, # here it is 223, define above \n",
    "        max_total_size = yolo_max_boxes,\n",
    "        iou_threshold = yolo_iou_threshold, # threshold for filtering the boxes\n",
    "        score_threshold = yolo_score_threshold # threshold for objectness score below which we ignore that bounding box\n",
    "    )\n",
    "\n",
    "    return boxes, scores, classes, valid_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eWEbU40RHoe",
    "papermill": {
     "duration": 0.05073,
     "end_time": "2022-03-20T16:50:19.647324",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.596594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Backbone Darknet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:19.762785Z",
     "iopub.status.busy": "2022-03-20T16:50:19.761952Z",
     "iopub.status.idle": "2022-03-20T16:50:19.763981Z",
     "shell.execute_reply": "2022-03-20T16:50:19.764391Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.871266Z"
    },
    "id": "e9Uj5IQCo2Wo",
    "papermill": {
     "duration": 0.064321,
     "end_time": "2022-03-20T16:50:19.764536",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.700215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def YoloV3(size=None, channels=3, anchors=yolo_anchors,\n",
    "           masks=yolo_anchor_masks, classes=80, training=False):\n",
    "    x = inputs = Input([size, size, channels], name='input') # input of an image\n",
    "\n",
    "    x_36, x_61, x = Darknet(name='yolo_darknet')(x) # backbone networks 3 outputs w.r.t to each grid size\n",
    "    # till here darknet network\n",
    "\n",
    "    # from below it's a Feature Pyramind Network with lateral connections\n",
    "    # for 13*13 grid size output\n",
    "    x = YoloConv(512, name='yolo_conv_0')(x) \n",
    "    output_0 = YoloOutput(512, len(masks[0]), classes, name='yolo_output_0')(x)\n",
    "    \n",
    "    # for 26*26 grid size output\n",
    "    x = YoloConv(256, name='yolo_conv_1')((x, x_61))\n",
    "    output_1 = YoloOutput(256, len(masks[1]), classes, name='yolo_output_1')(x)\n",
    "\n",
    "    # for 52*52 grid size output\n",
    "    x = YoloConv(128, name='yolo_conv_2')((x, x_36))\n",
    "    output_2 = YoloOutput(128, len(masks[2]), classes, name='yolo_output_2')(x)\n",
    "\n",
    "    if training:\n",
    "        return Model(inputs, (output_0, output_1, output_2), name='yolov3')\n",
    "\n",
    "    # for 13*13 grid size output\n",
    "    boxes_0 = Lambda(lambda x: yolo_boxes(x, anchors[masks[0]], classes),\n",
    "                     name='yolo_boxes_0')(output_0)\n",
    "    # for 26*26 grid size output\n",
    "    boxes_1 = Lambda(lambda x: yolo_boxes(x, anchors[masks[1]], classes),\n",
    "                     name='yolo_boxes_1')(output_1)\n",
    "    # for 52*52 grid size output\n",
    "    boxes_2 = Lambda(lambda x: yolo_boxes(x, anchors[masks[2]], classes),\n",
    "                     name='yolo_boxes_2')(output_2)\n",
    "    #  after combining boxes from various scales we have total 10,647 boxes which is too large\n",
    "    # so to remove invalid boxes we use non_maximum_suppression \n",
    "\n",
    "    outputs = Lambda(lambda x: yolo_nms(x, anchors, masks, classes),\n",
    "                     name='yolo_nms')((boxes_0[:3], boxes_1[:3], boxes_2[:3]))\n",
    "\n",
    "    return Model(inputs, outputs, name='yolov3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KptixCdtRNaE",
    "papermill": {
     "duration": 0.051014,
     "end_time": "2022-03-20T16:50:19.866260",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.815246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:19.986059Z",
     "iopub.status.busy": "2022-03-20T16:50:19.985031Z",
     "iopub.status.idle": "2022-03-20T16:50:20.011252Z",
     "shell.execute_reply": "2022-03-20T16:50:20.012420Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.888274Z"
    },
    "id": "VbJmFhlBo2Ub",
    "papermill": {
     "duration": 0.095792,
     "end_time": "2022-03-20T16:50:20.012648",
     "exception": false,
     "start_time": "2022-03-20T16:50:19.916856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def YoloLoss(anchors, classes=80, ignore_thresh=0.5):\n",
    "    def yolo_loss(y_true, y_pred):\n",
    "        # 1. transform all pred outputs\n",
    "        # y_pred: (batch_size, grid, grid, anchors, (x, y, w, h, obj, ...cls))\n",
    "        pred_box, pred_obj, pred_class, pred_xywh = yolo_boxes(\n",
    "            y_pred, anchors, classes)\n",
    "        pred_xy = pred_xywh[..., 0:2]\n",
    "        pred_wh = pred_xywh[..., 2:4]\n",
    "\n",
    "        # 2. transform all true outputs\n",
    "        # y_true: (batch_size, grid, grid, anchors, (x1, y1, x2, y2, obj, cls))\n",
    "        \n",
    "        true_box, true_obj, true_class_idx = tf.split(\n",
    "            y_true, (4, 1, 1), axis=-1) \n",
    "\n",
    "        # the above split function split (x1,y1,x2...cls) into (x1,y1),(x2,y2),(obj),(cls)\n",
    "        # the 4,1,1 is a length at which it split\n",
    "\n",
    "        true_xy = (true_box[..., 0:2] + true_box[..., 2:4]) / 2 # finding center (Xcen,Ycen)\n",
    "        true_wh = true_box[..., 2:4] - true_box[..., 0:2] # width and height\n",
    "\n",
    "        # give higher weights to small boxes\n",
    "        box_loss_scale = 2 - true_wh[..., 0] * true_wh[..., 1]\n",
    "\n",
    "        # 3. inverting the pred box equations\n",
    "        grid_size = tf.shape(y_true)[1]\n",
    "        grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))\n",
    "        grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)\n",
    "        true_xy = true_xy * tf.cast(grid_size, tf.float32) - \\\n",
    "            tf.cast(grid, tf.float32) # this code snippet giving us at which point each cell is starting and ending\n",
    "            # in resize image of 416 * 416 \n",
    "            # suppose there 13*13 = 169 cells , so every cell we will have starting and ending point\n",
    "        true_wh = tf.math.log(true_wh / anchors) \n",
    "        # YOLO doesn’t predict the absolute coordinates of the bounding box’s center\n",
    "        true_wh = tf.where(tf.math.is_inf(true_wh),\n",
    "                           tf.zeros_like(true_wh), true_wh)\n",
    "\n",
    "        # 4. calculate all masks\n",
    "        obj_mask = tf.squeeze(true_obj, -1)\n",
    "        \n",
    "        # ignore false positive when iou is over threshold\n",
    "        best_iou = tf.map_fn(\n",
    "            lambda x: tf.reduce_max(broadcast_iou(x[0], tf.boolean_mask(\n",
    "                x[1], tf.cast(x[2], tf.bool))), axis=-1),\n",
    "            (pred_box, true_box, obj_mask),\n",
    "            tf.float32)\n",
    "        ignore_mask = tf.cast(best_iou < ignore_thresh, tf.float32)\n",
    "\n",
    "        # 5. calculate all losses\n",
    "        xy_loss = obj_mask * box_loss_scale * \\\n",
    "            tf.reduce_sum(tf.square(true_xy - pred_xy), axis=-1)\n",
    "        wh_loss = obj_mask * box_loss_scale * \\\n",
    "            tf.reduce_sum(tf.square(true_wh - pred_wh), axis=-1)\n",
    "        \n",
    "        obj_loss = binary_crossentropy(true_obj, pred_obj)\n",
    "        # obj_loss = obj_mask * obj_loss + \\\n",
    "        #     (1 - obj_mask) * ignore_mask * obj_loss\n",
    "        \n",
    "        alpha = 0.75 # focal loss hyperparameter\n",
    "        conf_focal = tf.pow(obj_mask-tf.squeeze(tf.sigmoid(pred_obj),-1),2)\n",
    "        obj_loss = conf_focal*((1-alpha)*obj_mask*obj_loss + alpha*(1-obj_mask)*ignore_mask*obj_loss)  # batch * grid * grid * anchors_per_scale\n",
    "\n",
    "        # TODO: use binary_crossentropy instead\n",
    "        class_loss = obj_mask * sparse_categorical_crossentropy(\n",
    "            true_class_idx, pred_class)\n",
    "\n",
    "        # 6. sum over (batch, gridx, gridy, anchors) => (batch, 1)\n",
    "        xy_loss = tf.reduce_sum(xy_loss, axis=(1, 2, 3))\n",
    "        wh_loss = tf.reduce_sum(wh_loss, axis=(1, 2, 3))\n",
    "        obj_loss = tf.reduce_sum(obj_loss, axis=(1, 2, 3))\n",
    "        class_loss = tf.reduce_sum(class_loss, axis=(1, 2, 3))\n",
    "\n",
    "        return xy_loss + wh_loss + obj_loss + class_loss\n",
    "    return yolo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMF4aGyio4HA",
    "papermill": {
     "duration": 0.116266,
     "end_time": "2022-03-20T16:50:20.254324",
     "exception": false,
     "start_time": "2022-03-20T16:50:20.138058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:20.459980Z",
     "iopub.status.busy": "2022-03-20T16:50:20.459113Z",
     "iopub.status.idle": "2022-03-20T16:50:20.482769Z",
     "shell.execute_reply": "2022-03-20T16:50:20.483462Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.909714Z"
    },
    "id": "o5RVOsLZo3Wg",
    "papermill": {
     "duration": 0.127247,
     "end_time": "2022-03-20T16:50:20.483659",
     "exception": false,
     "start_time": "2022-03-20T16:50:20.356412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "YOLOV3_LAYER_LIST = ['yolo_darknet','yolo_conv_0','yolo_output_0','yolo_conv_1',\n",
    "                     'yolo_output_1','yolo_conv_2','yolo_output_2',]\n",
    "\n",
    "def load_darknet_weights(model, weights_file, tiny=False):\n",
    "    wf = open(weights_file, 'rb') # reading weights file\n",
    "    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n",
    "\n",
    "    layers = YOLOV3_LAYER_LIST\n",
    "\n",
    "    # iterating through all layers define in above yolov3_layers_list\n",
    "    for layer_name in layers:\n",
    "        # for eg if there is one layer darknet then there is many sub layers inside it's network\n",
    "        sub_model = model.get_layer(layer_name)\n",
    "        for i, layer in enumerate(sub_model.layers):\n",
    "            if not layer.name.startswith('conv2d'): \n",
    "                continue\n",
    "            batch_norm = None\n",
    "            if i + 1 < len(sub_model.layers) and \\\n",
    "                    sub_model.layers[i + 1].name.startswith('batch_norm'):\n",
    "                batch_norm = sub_model.layers[i + 1]\n",
    "\n",
    "            logging.info(\"{}/{} {}\".format(\n",
    "                sub_model.name, layer.name, 'bn' if batch_norm else 'bias'))\n",
    "\n",
    "            filters = layer.filters\n",
    "            size = layer.kernel_size[0]\n",
    "            in_dim = layer.input_shape[-1]\n",
    "\n",
    "            if batch_norm is None:\n",
    "                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n",
    "            else:\n",
    "                # darknet [beta, gamma, mean, variance]\n",
    "                bn_weights = np.fromfile(\n",
    "                    wf, dtype=np.float32, count=4 * filters)\n",
    "                # tf [gamma, beta, mean, variance]\n",
    "                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n",
    "\n",
    "            # darknet shape (out_dim, in_dim, height, width)\n",
    "            conv_shape = (filters, in_dim, size, size)\n",
    "            conv_weights = np.fromfile(\n",
    "                wf, dtype=np.float32, count=np.product(conv_shape))\n",
    "            # tf shape (height, width, in_dim, out_dim)\n",
    "            conv_weights = conv_weights.reshape(\n",
    "                conv_shape).transpose([2, 3, 1, 0])\n",
    "\n",
    "            if batch_norm is None:\n",
    "                layer.set_weights([conv_weights, conv_bias])\n",
    "            else:\n",
    "                layer.set_weights([conv_weights])\n",
    "                batch_norm.set_weights(bn_weights)\n",
    "\n",
    "    assert len(wf.read()) == 0, 'failed to read all data'\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "\n",
    "def broadcast_iou(box_1, box_2):\n",
    "    # box_1: (..., (x1, y1, x2, y2))\n",
    "    # box_2: (N, (x1, y1, x2, y2))\n",
    "\n",
    "    # broadcast boxes\n",
    "    box_1 = tf.expand_dims(box_1, -2)\n",
    "    box_2 = tf.expand_dims(box_2, 0)\n",
    "    # new_shape: (..., N, (x1, y1, x2, y2))\n",
    "    new_shape = tf.broadcast_dynamic_shape(tf.shape(box_1), tf.shape(box_2))\n",
    "    box_1 = tf.broadcast_to(box_1, new_shape) # it will change the shape of box into new_shape given\n",
    "    box_2 = tf.broadcast_to(box_2, new_shape)\n",
    "\n",
    "    # in below code we are finding intersection box width and height through which we will find intersection area.\n",
    "    # and this we are finding all boxes \n",
    "    int_w = tf.maximum(tf.minimum(box_1[..., 2], box_2[..., 2]) -\n",
    "                       tf.maximum(box_1[..., 0], box_2[..., 0]), 0) \n",
    "    int_h = tf.maximum(tf.minimum(box_1[..., 3], box_2[..., 3]) -\n",
    "                       tf.maximum(box_1[..., 1], box_2[..., 1]), 0) \n",
    "    int_area = int_w * int_h  # area of intersection\n",
    "    box_1_area = (box_1[..., 2] - box_1[..., 0]) * \\\n",
    "        (box_1[..., 3] - box_1[..., 1]) # this box_1_area contains all boxes area predicted in an image\n",
    "    box_2_area = (box_2[..., 2] - box_2[..., 0]) * \\\n",
    "        (box_2[..., 3] - box_2[..., 1]) # this box2_area is our ground truth box area\n",
    "\n",
    "    # Formula: Union(A,B) = A + B - Inter(A,B)\n",
    "    return int_area / (box_1_area + box_2_area - int_area)\n",
    "\n",
    "\n",
    "def freeze_all(model, frozen=True):\n",
    "    model.trainable = not frozen\n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        for l in model.layers:\n",
    "            freeze_all(l, frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI9WjPsvfZs2",
    "papermill": {
     "duration": 0.097456,
     "end_time": "2022-03-20T16:50:20.665656",
     "exception": false,
     "start_time": "2022-03-20T16:50:20.568200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Dataset Loader And Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:20.808349Z",
     "iopub.status.busy": "2022-03-20T16:50:20.807644Z",
     "iopub.status.idle": "2022-03-20T16:50:20.811700Z",
     "shell.execute_reply": "2022-03-20T16:50:20.812496Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.931608Z"
    },
    "id": "ylcMnsTSVYtV",
    "papermill": {
     "duration": 0.075792,
     "end_time": "2022-03-20T16:50:20.812658",
     "exception": false,
     "start_time": "2022-03-20T16:50:20.736866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def transform_targets_for_output(y_true, grid_size, anchor_idxs):\n",
    "    # y_true: (N, boxes, (x1, y1, x2, y2, class, best_anchor))\n",
    "    N = tf.shape(y_true)[0]\n",
    "\n",
    "    # y_true_out: (N, grid, grid, anchors, [x, y, w, h, obj, class])\n",
    "    y_true_out = tf.zeros(\n",
    "        (N, grid_size, grid_size, tf.shape(anchor_idxs)[0], 6))\n",
    "\n",
    "    anchor_idxs = tf.cast(anchor_idxs, tf.int32)\n",
    "\n",
    "    indexes = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "    idx = 0\n",
    "\n",
    "    # below iteration change the values and update it to the format which acceptable by yolov3.\n",
    "    for i in tf.range(N):\n",
    "        for j in tf.range(tf.shape(y_true)[1]):\n",
    "            if tf.equal(y_true[i][j][2], 0): \n",
    "                continue\n",
    "            anchor_eq = tf.equal(\n",
    "                anchor_idxs, tf.cast(y_true[i][j][5], tf.int32))\n",
    "\n",
    "            if tf.reduce_any(anchor_eq):\n",
    "                box = y_true[i][j][0:4] #(x1,y1,x2,y2)\n",
    "                box_xy = (y_true[i][j][0:2] + y_true[i][j][2:4]) / 2 # ((x1+x2)/2,(y1+y2)/2)\n",
    "\n",
    "                # which is (Xcenter,Ycenter)\n",
    "\n",
    "                anchor_idx = tf.cast(tf.where(anchor_eq), tf.int32)\n",
    "                grid_xy = tf.cast(box_xy // (1/grid_size), tf.int32) # multiplying it by grid_size\n",
    "\n",
    "                # grid[y][x][anchor] = (tx, ty, bw, bh, obj, class)\n",
    "                indexes = indexes.write(\n",
    "                    idx, [i, grid_xy[1], grid_xy[0], anchor_idx[0][0]]) \n",
    "                updates = updates.write(\n",
    "                    idx, [box[0], box[1], box[2], box[3], 1, y_true[i][j][4]])\n",
    "                idx += 1\n",
    "    return tf.tensor_scatter_nd_update(\n",
    "        y_true_out, indexes.stack(), updates.stack())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:20.936803Z",
     "iopub.status.busy": "2022-03-20T16:50:20.936190Z",
     "iopub.status.idle": "2022-03-20T16:50:20.940540Z",
     "shell.execute_reply": "2022-03-20T16:50:20.940058Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.947576Z"
    },
    "id": "UXjrP_rXVYlb",
    "papermill": {
     "duration": 0.070353,
     "end_time": "2022-03-20T16:50:20.940669",
     "exception": false,
     "start_time": "2022-03-20T16:50:20.870316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_targets(y_train, anchors, anchor_masks, size):\n",
    "    y_outs = []\n",
    "    grid_size = size // 32 # suppose we input 416 size then grid size is 416//32 = 13\n",
    "\n",
    "    # calculate anchor index for true boxes\n",
    "    anchors = tf.cast(anchors, tf.float32) # casting every anchors to float\n",
    "    anchor_area = anchors[..., 0] * anchors[..., 1] # calculating the area of anchors\n",
    "    box_wh = y_train[..., 2:4] - y_train[..., 0:2] # here we are peforming xmax-xmin,ymax-ymin using vectors\n",
    "    box_wh = tf.tile(tf.expand_dims(box_wh, -2),\n",
    "                     (1, 1, tf.shape(anchors)[0], 1))\n",
    "    box_area = box_wh[..., 0] * box_wh[..., 1] # these are our Ground Truth Box Area\n",
    "    intersection = tf.minimum(box_wh[..., 0], anchors[..., 0]) * \\\n",
    "        tf.minimum(box_wh[..., 1], anchors[..., 1]) # here we tring to get IOU area \n",
    "    iou = intersection / (box_area + anchor_area - intersection) # simple operation of Intersection/Union\n",
    "    anchor_idx = tf.cast(tf.argmax(iou, axis=-1), tf.float32) # storing those anchor index which has highest IOU number\n",
    "    anchor_idx = tf.expand_dims(anchor_idx, axis=-1)\n",
    "\n",
    "    y_train = tf.concat([y_train, anchor_idx], axis=-1)\n",
    "\n",
    "    for anchor_idxs in anchor_masks:\n",
    "        y_outs.append(transform_targets_for_output(\n",
    "            y_train, grid_size, anchor_idxs))\n",
    "        grid_size *= 2 # here we are calling the above function for 13*13 grid then, 26*26 grid then, 52*52 grid.\n",
    "\n",
    "    return tuple(y_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_Z9P9tgx_me",
    "papermill": {
     "duration": 0.056736,
     "end_time": "2022-03-20T16:50:21.054664",
     "exception": false,
     "start_time": "2022-03-20T16:50:20.997928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Loading Yolov3 weights into Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:21.172665Z",
     "iopub.status.busy": "2022-03-20T16:50:21.171708Z",
     "iopub.status.idle": "2022-03-20T16:50:25.536424Z",
     "shell.execute_reply": "2022-03-20T16:50:25.537370Z",
     "shell.execute_reply.started": "2022-03-20T11:14:15.9607Z"
    },
    "id": "B_GA8Ytb0KE-",
    "outputId": "03313e25-b686-4220-ba08-6d33e67e5a76",
    "papermill": {
     "duration": 4.426737,
     "end_time": "2022-03-20T16:50:25.537570",
     "exception": false,
     "start_time": "2022-03-20T16:50:21.110833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 16:50:21.265052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:21.373764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:21.374754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:21.376131: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-20 16:50:21.376883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:21.377617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:21.378328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:23.298365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:23.299166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:23.299808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-20 16:50:23.300405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"yolov3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "yolo_darknet (Functional)       ((None, None, None,  40620640    input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_0 (Functional)        (None, None, None, 5 11024384    yolo_darknet[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_1 (Functional)        (None, None, None, 2 2957312     yolo_conv_0[0][0]                \n",
      "                                                                 yolo_darknet[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_conv_2 (Functional)        (None, None, None, 1 741376      yolo_conv_1[0][0]                \n",
      "                                                                 yolo_darknet[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_0 (Functional)      (None, None, None, 3 4984063     yolo_conv_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_1 (Functional)      (None, None, None, 3 1312511     yolo_conv_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_2 (Functional)      (None, None, None, 3 361471      yolo_conv_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_0 (Lambda)           ((None, None, None,  0           yolo_output_0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_1 (Lambda)           ((None, None, None,  0           yolo_output_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_boxes_2 (Lambda)           ((None, None, None,  0           yolo_output_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "yolo_nms (Lambda)               ((None, 116, 4), (No 0           yolo_boxes_0[0][0]               \n",
      "                                                                 yolo_boxes_0[0][1]               \n",
      "                                                                 yolo_boxes_0[0][2]               \n",
      "                                                                 yolo_boxes_1[0][0]               \n",
      "                                                                 yolo_boxes_1[0][1]               \n",
      "                                                                 yolo_boxes_1[0][2]               \n",
      "                                                                 yolo_boxes_2[0][0]               \n",
      "                                                                 yolo_boxes_2[0][1]               \n",
      "                                                                 yolo_boxes_2[0][2]               \n",
      "==================================================================================================\n",
      "Total params: 62,001,757\n",
      "Trainable params: 61,949,149\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 2.48 s, sys: 582 ms, total: 3.07 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "yolo = YoloV3(classes=80)\n",
    "yolo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:25.647858Z",
     "iopub.status.busy": "2022-03-20T16:50:25.647310Z",
     "iopub.status.idle": "2022-03-20T16:50:28.932416Z",
     "shell.execute_reply": "2022-03-20T16:50:28.931883Z",
     "shell.execute_reply.started": "2022-03-20T11:14:19.99516Z"
    },
    "papermill": {
     "duration": 3.342092,
     "end_time": "2022-03-20T16:50:28.932563",
     "exception": false,
     "start_time": "2022-03-20T16:50:25.590471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_darknet_weights(yolo,\"/kaggle/input/pre-trained-weights/yolov3.weights\", False)\n",
    "yolo.save_weights(\"/kaggle/working/yolov3.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:29.047801Z",
     "iopub.status.busy": "2022-03-20T16:50:29.047005Z",
     "iopub.status.idle": "2022-03-20T16:50:29.732413Z",
     "shell.execute_reply": "2022-03-20T16:50:29.731856Z",
     "shell.execute_reply.started": "2022-03-20T11:14:24.551845Z"
    },
    "papermill": {
     "duration": 0.746039,
     "end_time": "2022-03-20T16:50:29.732542",
     "exception": false,
     "start_time": "2022-03-20T16:50:28.986503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  checkpoint\tyolov3.tf.data-00000-of-00001  yolov3.tf.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHrmlXC_R1x3",
    "papermill": {
     "duration": 0.053377,
     "end_time": "2022-03-20T16:50:29.838329",
     "exception": false,
     "start_time": "2022-03-20T16:50:29.784952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Parsing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:29.955340Z",
     "iopub.status.busy": "2022-03-20T16:50:29.954411Z",
     "iopub.status.idle": "2022-03-20T16:50:29.956212Z",
     "shell.execute_reply": "2022-03-20T16:50:29.956610Z",
     "shell.execute_reply.started": "2022-03-20T11:14:25.254173Z"
    },
    "id": "VsMdGHyGR0h9",
    "papermill": {
     "duration": 0.065964,
     "end_time": "2022-03-20T16:50:29.956763",
     "exception": false,
     "start_time": "2022-03-20T16:50:29.890799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_dataset(data,class_dict,size,image,path,yolo_max_boxes,count=0):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for img in tqdm(image):\n",
    "        x_train = Image.open(path+img) # reading image\n",
    "        width,height = x_train.size # storing actual width and height so that we can later scale it\n",
    "        x_train = x_train.resize((size,size)) # resizing \n",
    "        x_train = np.array(x_train)\n",
    "        temp_data = []\n",
    "        # ierating over dataset having info about objects in an image\n",
    "        for _,row in data[data['img_path']==path+img].iterrows():\n",
    "            xmin = row.min_w/width\n",
    "            xmax = row.max_w/width\n",
    "            ymin = row.min_h/height\n",
    "            ymax = row.max_h/height\n",
    "            cls = class_dict[row.category]\n",
    "            temp_data.append([xmin,ymin,xmax,ymax,cls])\n",
    "        temp_data = temp_data+[[0,0,0,0,0]]*(yolo_max_boxes-len(temp_data)) # it's like padding \n",
    "        #return(temp)\n",
    "        Y.append(temp_data)\n",
    "        X.append(x_train)\n",
    "    return(np.array(X),np.stack(np.array(Y)))\n",
    "\n",
    "# transforming each image and normalizing it in range [0,1]\n",
    "def transform_images(x,size):\n",
    "    x = tf.image.resize(x,(size,size))\n",
    "    x = x/255.0\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BG6RgO3SUMM",
    "papermill": {
     "duration": 0.053139,
     "end_time": "2022-03-20T16:50:30.066260",
     "exception": false,
     "start_time": "2022-03-20T16:50:30.013121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading Train  and Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:50:30.180474Z",
     "iopub.status.busy": "2022-03-20T16:50:30.179602Z",
     "iopub.status.idle": "2022-03-20T16:52:44.521983Z",
     "shell.execute_reply": "2022-03-20T16:52:44.521506Z",
     "shell.execute_reply.started": "2022-03-20T11:14:25.2734Z"
    },
    "id": "2IA5kCAon86D",
    "outputId": "20975c22-c524-4f30-8717-6d92b07213f9",
    "papermill": {
     "duration": 134.404114,
     "end_time": "2022-03-20T16:52:44.522130",
     "exception": false,
     "start_time": "2022-03-20T16:50:30.118016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [02:06<00:00, 19.77it/s]\n",
      "2022-03-20 16:52:37.971216: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3072000000 exceeds 10% of free system memory.\n",
      "2022-03-20 16:52:40.976330: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3072000000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "x,y= parse_dataset(\n",
    "   df,class_dict,size,train_image[:],'/kaggle/input/global-wheat-detection/train/',116) \n",
    "# df = data\n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=64) # Randomizing the data\n",
    "train_dataset = train_dataset.batch(8) # Setting Batch size\n",
    "train_dataset = train_dataset.map(lambda x, y: (transform_images(x, size),\n",
    "                                  transform_targets(y, yolo_anchors, yolo_anchor_masks, size)))\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE) # Prefetching for faster performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:52:45.169400Z",
     "iopub.status.busy": "2022-03-20T16:52:45.168487Z",
     "iopub.status.idle": "2022-03-20T16:52:45.171402Z",
     "shell.execute_reply": "2022-03-20T16:52:45.171808Z",
     "shell.execute_reply.started": "2022-03-20T11:16:50.650032Z"
    },
    "id": "lSEM6gQFk-7C",
    "outputId": "fb7fbc3b-5cf6-4218-cf40-4ca00c0ca505",
    "papermill": {
     "duration": 0.329405,
     "end_time": "2022-03-20T16:52:45.171943",
     "exception": false,
     "start_time": "2022-03-20T16:52:44.842538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 320, 320, 3), ((None, 10, 10, 3, 6), (None, 20, 20, 3, 6), (None, 40, 40, 3, 6))), types: (tf.float32, (tf.float32, tf.float32, tf.float32))>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:52:45.833167Z",
     "iopub.status.busy": "2022-03-20T16:52:45.832249Z",
     "iopub.status.idle": "2022-03-20T16:53:32.981478Z",
     "shell.execute_reply": "2022-03-20T16:53:32.980945Z",
     "shell.execute_reply.started": "2022-03-20T11:16:50.661575Z"
    },
    "id": "jV0oCnpHlSxz",
    "outputId": "5c347615-606a-4cf3-e096-48f704549786",
    "papermill": {
     "duration": 47.486648,
     "end_time": "2022-03-20T16:53:32.981606",
     "exception": false,
     "start_time": "2022-03-20T16:52:45.494958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 873/873 [00:44<00:00, 19.56it/s]\n",
      "2022-03-20 16:53:30.988146: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1072742400 exceeds 10% of free system memory.\n",
      "2022-03-20 16:53:32.149338: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1072742400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# parsing valid dataset \n",
    "x,y= parse_dataset(df,class_dict,size,test_image[:],'/kaggle/input/global-wheat-detection/train/',116) \n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "val_dataset = val_dataset.shuffle(buffer_size=16)\n",
    "val_dataset = val_dataset.batch(8)\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x, y: (transform_images(x, size),\n",
    "    transform_targets(y, yolo_anchors, yolo_anchor_masks, size)))\n",
    "\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:33.831661Z",
     "iopub.status.busy": "2022-03-20T16:53:33.830415Z",
     "iopub.status.idle": "2022-03-20T16:53:33.833977Z",
     "shell.execute_reply": "2022-03-20T16:53:33.834558Z",
     "shell.execute_reply.started": "2022-03-20T11:17:41.430954Z"
    },
    "id": "db7xNBOLd73p",
    "outputId": "f40cf3ea-7380-4afd-9808-e76c54e46fec",
    "papermill": {
     "duration": 0.436143,
     "end_time": "2022-03-20T16:53:33.834718",
     "exception": false,
     "start_time": "2022-03-20T16:53:33.398575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 320, 320, 3), ((None, 10, 10, 3, 6), (None, 20, 20, 3, 6), (None, 40, 40, 3, 6))), types: (tf.float32, (tf.float32, tf.float32, tf.float32))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BPDSk7ilypC",
    "papermill": {
     "duration": 0.415562,
     "end_time": "2022-03-20T16:53:34.693312",
     "exception": false,
     "start_time": "2022-03-20T16:53:34.277750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:35.546655Z",
     "iopub.status.busy": "2022-03-20T16:53:35.545449Z",
     "iopub.status.idle": "2022-03-20T16:53:37.345980Z",
     "shell.execute_reply": "2022-03-20T16:53:37.344932Z",
     "shell.execute_reply.started": "2022-03-20T11:17:41.438694Z"
    },
    "id": "pqSs1z65Vkta",
    "papermill": {
     "duration": 2.225654,
     "end_time": "2022-03-20T16:53:37.346118",
     "exception": false,
     "start_time": "2022-03-20T16:53:35.120464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = YoloV3(size, training=True, classes=num_classes)\n",
    "anchors = yolo_anchors\n",
    "anchor_masks = yolo_anchor_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG6q3NijdGhX",
    "papermill": {
     "duration": 0.616826,
     "end_time": "2022-03-20T16:53:38.385470",
     "exception": false,
     "start_time": "2022-03-20T16:53:37.768644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading Pretrained model and freezing for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:39.306074Z",
     "iopub.status.busy": "2022-03-20T16:53:39.305136Z",
     "iopub.status.idle": "2022-03-20T16:53:42.216837Z",
     "shell.execute_reply": "2022-03-20T16:53:42.217297Z",
     "shell.execute_reply.started": "2022-03-20T11:17:43.037578Z"
    },
    "id": "KQZETfM8VYjy",
    "outputId": "ac6705bc-ac5f-47ed-e612-0b1d27e6d88f",
    "papermill": {
     "duration": 3.339395,
     "end_time": "2022-03-20T16:53:42.217452",
     "exception": false,
     "start_time": "2022-03-20T16:53:38.878057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.77 s, sys: 179 ms, total: 2.95 s\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_pretrained = YoloV3(size, training=True, classes=80)\n",
    "model_pretrained.load_weights(\"/kaggle/working/yolov3.tf\")\n",
    "model.get_layer('yolo_darknet').set_weights(\n",
    "model_pretrained.get_layer('yolo_darknet').get_weights())\n",
    "freeze_all(model.get_layer('yolo_darknet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYARhdfOdASu",
    "papermill": {
     "duration": 0.417987,
     "end_time": "2022-03-20T16:53:43.055574",
     "exception": false,
     "start_time": "2022-03-20T16:53:42.637587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Defining Optimizer and Loss for the YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:43.903003Z",
     "iopub.status.busy": "2022-03-20T16:53:43.901375Z",
     "iopub.status.idle": "2022-03-20T16:53:43.903622Z",
     "shell.execute_reply": "2022-03-20T16:53:43.904019Z",
     "shell.execute_reply.started": "2022-03-20T11:17:46.530039Z"
    },
    "id": "C7SjghvhVYhJ",
    "papermill": {
     "duration": 0.429029,
     "end_time": "2022-03-20T16:53:43.904171",
     "exception": false,
     "start_time": "2022-03-20T16:53:43.475142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " # we are using graph mode of tensorflow so that we can use our own Gradient Tape\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # Adam optimizers\n",
    "loss = [YoloLoss(anchors[mask], classes=num_classes) # customized yolo loss define above in utils.\n",
    "            for mask in anchor_masks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOxjIUzNCDTf",
    "papermill": {
     "duration": 0.417927,
     "end_time": "2022-03-20T16:53:44.744392",
     "exception": false,
     "start_time": "2022-03-20T16:53:44.326465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Calculating Loss using Gradient Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:45.589237Z",
     "iopub.status.busy": "2022-03-20T16:53:45.588600Z",
     "iopub.status.idle": "2022-03-20T16:53:45.594684Z",
     "shell.execute_reply": "2022-03-20T16:53:45.594265Z",
     "shell.execute_reply.started": "2022-03-20T11:17:46.537447Z"
    },
    "id": "f3rozfk5qzMe",
    "papermill": {
     "duration": 0.431488,
     "end_time": "2022-03-20T16:53:45.594802",
     "exception": false,
     "start_time": "2022-03-20T16:53:45.163314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "train_log_dir = '/kaggle/working/320_logs/gradient_tape/' + current_time + '/train' # train dir path\n",
    "test_log_dir = '/kaggle/working/320_logs/gradient_tape/' + current_time + '/test' # test dir path\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir) # train writer\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir) # test writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T16:53:46.505335Z",
     "iopub.status.busy": "2022-03-20T16:53:46.504439Z",
     "iopub.status.idle": "2022-03-21T01:09:08.340853Z",
     "shell.execute_reply": "2022-03-21T01:09:08.342321Z",
     "shell.execute_reply.started": "2022-03-20T11:17:46.550002Z"
    },
    "id": "fM3EjboZfVq4",
    "outputId": "7e4e65a4-bccc-4e9b-dbef-5823561a8485",
    "papermill": {
     "duration": 29722.329094,
     "end_time": "2022-03-21T01:09:08.343302",
     "exception": false,
     "start_time": "2022-03-20T16:53:46.014208",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-20 16:53:46.529843: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 3072000000 exceeds 10% of free system memory.\n",
      "2022-03-20 16:53:49.888959: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-03-20 16:53:51.776727: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, train: 2202.5546875, val: 596.8167114257812\n",
      "1, train: 374.0102233886719, val: 262.29541015625\n",
      "2, train: 208.35400390625, val: 154.3477783203125\n",
      "3, train: 130.82810974121094, val: 104.08112335205078\n",
      "Saved checkpoint for step 5: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-1\n",
      "4, train: 91.29635620117188, val: 78.09040832519531\n",
      "5, train: 67.37816619873047, val: 57.37989044189453\n",
      "6, train: 52.17287826538086, val: 45.503028869628906\n",
      "7, train: 41.804115295410156, val: 37.16824722290039\n",
      "8, train: 34.480037689208984, val: 31.139345169067383\n",
      "Saved checkpoint for step 10: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-2\n",
      "9, train: 29.165685653686523, val: 26.69731903076172\n",
      "10, train: 25.23693084716797, val: 23.357187271118164\n",
      "11, train: 22.24479866027832, val: 20.79964256286621\n",
      "12, train: 19.94696617126465, val: 18.816442489624023\n",
      "13, train: 18.144184112548828, val: 17.24163246154785\n",
      "Saved checkpoint for step 15: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-3\n",
      "14, train: 16.718778610229492, val: 16.03485870361328\n",
      "15, train: 15.573565483093262, val: 15.010744094848633\n",
      "16, train: 14.65135669708252, val: 14.197611808776855\n",
      "17, train: 13.903261184692383, val: 13.543427467346191\n",
      "18, train: 13.292404174804688, val: 12.991389274597168\n",
      "Saved checkpoint for step 20: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-4\n",
      "19, train: 12.784579277038574, val: 12.547133445739746\n",
      "20, train: 12.361112594604492, val: 12.152730941772461\n",
      "21, train: 12.006638526916504, val: 11.835729598999023\n",
      "22, train: 11.707290649414062, val: 11.562446594238281\n",
      "23, train: 11.448972702026367, val: 11.328001976013184\n",
      "Saved checkpoint for step 25: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-5\n",
      "24, train: 11.226751327514648, val: 11.122284889221191\n",
      "25, train: 11.03168773651123, val: 10.937848091125488\n",
      "26, train: 10.855989456176758, val: 10.773180961608887\n",
      "27, train: 10.697772979736328, val: 10.62197494506836\n",
      "28, train: 10.55111312866211, val: 10.48339557647705\n",
      "Saved checkpoint for step 30: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-6\n",
      "29, train: 10.412432670593262, val: 10.349241256713867\n",
      "30, train: 10.279430389404297, val: 10.217619895935059\n",
      "31, train: 10.14944076538086, val: 10.088939666748047\n",
      "32, train: 10.020647048950195, val: 9.960982322692871\n",
      "33, train: 9.890410423278809, val: 9.829476356506348\n",
      "Saved checkpoint for step 35: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-7\n",
      "34, train: 9.758003234863281, val: 9.696247100830078\n",
      "35, train: 9.622232437133789, val: 9.558525085449219\n",
      "36, train: 9.481841087341309, val: 9.415220260620117\n",
      "37, train: 9.336166381835938, val: 9.267406463623047\n",
      "38, train: 9.184855461120605, val: 9.112994194030762\n",
      "Saved checkpoint for step 40: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-8\n",
      "39, train: 9.027870178222656, val: 8.954866409301758\n",
      "40, train: 8.8652982711792, val: 8.78869342803955\n",
      "41, train: 8.698308944702148, val: 8.619643211364746\n",
      "42, train: 8.527992248535156, val: 8.449197769165039\n",
      "43, train: 9.484362602233887, val: 8.452658653259277\n",
      "Saved checkpoint for step 45: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-9\n",
      "44, train: 8.375032424926758, val: 8.324911117553711\n",
      "45, train: 8.266307830810547, val: 8.2191743850708\n",
      "46, train: 8.164629936218262, val: 8.12062931060791\n",
      "47, train: 8.068215370178223, val: 8.027131080627441\n",
      "48, train: 7.976886749267578, val: 7.938324928283691\n",
      "Saved checkpoint for step 50: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-10\n",
      "49, train: 7.891624927520752, val: 7.856543064117432\n",
      "50, train: 7.811948776245117, val: 7.779740333557129\n",
      "51, train: 7.737979412078857, val: 7.708164691925049\n",
      "52, train: 7.6693501472473145, val: 7.641269207000732\n",
      "53, train: 7.604907035827637, val: 7.583717346191406\n",
      "Saved checkpoint for step 55: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-11\n",
      "54, train: 8.694578170776367, val: 7.647049427032471\n",
      "55, train: 7.611423492431641, val: 7.593944072723389\n",
      "56, train: 7.578723907470703, val: 7.565300941467285\n",
      "57, train: 7.548445701599121, val: 7.534250259399414\n",
      "58, train: 7.518935203552246, val: 7.503489017486572\n",
      "Saved checkpoint for step 60: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-12\n",
      "59, train: 7.4886393547058105, val: 7.473172664642334\n",
      "60, train: 7.457418918609619, val: 7.441158294677734\n",
      "61, train: 7.425251007080078, val: 7.4089860916137695\n",
      "62, train: 7.39082145690918, val: 7.373684406280518\n",
      "63, train: 7.354785442352295, val: 7.33657693862915\n",
      "Saved checkpoint for step 65: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-13\n",
      "64, train: 7.317531585693359, val: 7.2991623878479\n",
      "65, train: 7.279059886932373, val: 7.260138034820557\n",
      "66, train: 7.239544868469238, val: 7.219563007354736\n",
      "67, train: 7.206212043762207, val: 7.489075183868408\n",
      "68, train: 7.165550708770752, val: 7.144935131072998\n",
      "Saved checkpoint for step 70: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-14\n",
      "69, train: 7.124775409698486, val: 7.1045637130737305\n",
      "70, train: 7.084925651550293, val: 7.065185070037842\n",
      "71, train: 7.04607629776001, val: 7.027106285095215\n",
      "72, train: 7.008596420288086, val: 6.990359306335449\n",
      "73, train: 6.972796440124512, val: 6.955617904663086\n",
      "Saved checkpoint for step 75: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-15\n",
      "74, train: 6.938988208770752, val: 6.922807216644287\n",
      "75, train: 6.9073991775512695, val: 6.892444610595703\n",
      "76, train: 6.878190040588379, val: 6.865041255950928\n",
      "77, train: 6.851465702056885, val: 6.8392109870910645\n",
      "78, train: 6.827253818511963, val: 6.816195964813232\n",
      "Saved checkpoint for step 80: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-16\n",
      "79, train: 6.805515766143799, val: 6.795814514160156\n",
      "80, train: 6.786203861236572, val: 6.777795791625977\n",
      "81, train: 6.769165515899658, val: 6.761775970458984\n",
      "82, train: 6.754237174987793, val: 6.748035907745361\n",
      "83, train: 6.741234302520752, val: 6.735814094543457\n",
      "Saved checkpoint for step 85: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-17\n",
      "84, train: 6.729917526245117, val: 6.725462913513184\n",
      "85, train: 6.720126152038574, val: 6.716309070587158\n",
      "86, train: 6.711671352386475, val: 6.708905220031738\n",
      "87, train: 6.704372406005859, val: 6.701629161834717\n",
      "88, train: 6.6979451179504395, val: 6.695586204528809\n",
      "Saved checkpoint for step 90: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-18\n",
      "89, train: 6.69249963760376, val: 6.690425872802734\n",
      "90, train: 6.687515735626221, val: 6.685649394989014\n",
      "91, train: 6.68322229385376, val: 6.681315898895264\n",
      "92, train: 6.679433822631836, val: 6.677737236022949\n",
      "93, train: 7.670156478881836, val: 6.8396477699279785\n",
      "Saved checkpoint for step 95: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-19\n",
      "94, train: 6.809886932373047, val: 6.802318572998047\n",
      "95, train: 6.798653602600098, val: 6.794528961181641\n",
      "96, train: 6.793570041656494, val: 6.790459156036377\n",
      "97, train: 6.789948463439941, val: 6.787456035614014\n",
      "98, train: 6.786744117736816, val: 6.784753322601318\n",
      "Saved checkpoint for step 100: /kaggle/320_checkpoints/working/yolov3_train/tf_ckpts/ckpt-20\n",
      "99, train: 6.784137725830078, val: 6.782240390777588\n",
      "100, train: 6.7813591957092285, val: 6.779543399810791\n"
     ]
    }
   ],
   "source": [
    "avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model = model)\n",
    "manager = tf.train.CheckpointManager(ckpt, '/kaggle/320_checkpoints/working/yolov3_train/tf_ckpts', max_to_keep=3)\n",
    "ckpt.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    start = ckpt.step.numpy() \n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "    start = 0\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(start, epochs+1):\n",
    "    for batch, (images, labels) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(images, training=True)\n",
    "            regularization_loss = tf.reduce_sum(model.losses)\n",
    "            pred_loss = []\n",
    "            for output, label, loss_fn in zip(outputs, labels, loss):\n",
    "                pred_loss.append(loss_fn(label, output))\n",
    "            total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n",
    "        # calculating grads over trainable parameters\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables) # calculating loss after each batch \n",
    "        optimizer.apply_gradients(\n",
    "            zip(grads, model.trainable_variables)) # then appliying gradient optimization on the loss to fine tune the weights \n",
    "\n",
    "        # writing summary to train file(for tensorboard)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('avg_loss', total_loss.numpy(), step=epoch)\n",
    "        # to update avg loss after each batch.\n",
    "        avg_loss.update_state(total_loss)\n",
    "    \n",
    "    # testing datasets\n",
    "    for batch, (images, labels) in enumerate(val_dataset):\n",
    "        outputs = model(images)\n",
    "        regularization_loss = tf.reduce_sum(model.losses)\n",
    "        pred_loss = []\n",
    "        for output, label, loss_fn in zip(outputs, labels, loss):\n",
    "            pred_loss.append(loss_fn(label, output))\n",
    "        total_loss = tf.reduce_sum(pred_loss) + regularization_loss\n",
    "\n",
    "        # writing summary to test file(for tensorboard)\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('avg_val_loss', total_loss.numpy(), step=epoch)\n",
    "        avg_val_loss.update_state(total_loss)\n",
    "\n",
    "    # print result \n",
    "    print(\"{}, train: {}, val: {}\".format(epoch,\n",
    "        avg_loss.result().numpy(),\n",
    "        avg_val_loss.result().numpy()))\n",
    "    \n",
    "    ckpt.step.assign_add(1)\n",
    "    if int(ckpt.step) % 5 == 0:\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "\n",
    "    avg_loss.reset_states()\n",
    "    avg_val_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T01:09:09.345116Z",
     "iopub.status.busy": "2022-03-21T01:09:09.309255Z",
     "iopub.status.idle": "2022-03-21T01:09:09.982851Z",
     "shell.execute_reply": "2022-03-21T01:09:09.982379Z",
     "shell.execute_reply.started": "2022-03-20T11:37:32.026208Z"
    },
    "id": "6CZSiBdFIs7J",
    "papermill": {
     "duration": 1.182237,
     "end_time": "2022-03-21T01:09:09.982979",
     "exception": false,
     "start_time": "2022-03-21T01:09:08.800742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_weights('/kaggle/working/Model_Save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-21T01:09:10.973815Z",
     "iopub.status.busy": "2022-03-21T01:09:10.896249Z",
     "iopub.status.idle": "2022-03-21T01:09:11.929610Z",
     "shell.execute_reply": "2022-03-21T01:09:11.929049Z",
     "shell.execute_reply.started": "2022-03-20T11:37:36.580987Z"
    },
    "papermill": {
     "duration": 1.490669,
     "end_time": "2022-03-21T01:09:11.929750",
     "exception": false,
     "start_time": "2022-03-21T01:09:10.439081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv  test  train  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlxJNSoGE6Wj",
    "papermill": {
     "duration": 0.456487,
     "end_time": "2022-03-21T01:09:18.362629",
     "exception": false,
     "start_time": "2022-03-21T01:09:17.906142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step by Step Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpkjSFofE9AD",
    "papermill": {
     "duration": 0.457574,
     "end_time": "2022-03-21T01:09:19.277296",
     "exception": false,
     "start_time": "2022-03-21T01:09:18.819722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Download the gloabl_wheat_detection zip from kaggle and unzip the data.\n",
    "2. Download the pretrained yolov3 weights.\n",
    "3. Preprocess the given dataframe by expanding the bbox dimensions and calculate x_min,y_min,x_max,y_max and save it.\n",
    "4. Split the data into train and validation data.\n",
    "5. Import all the necessary modules that are needed.\n",
    "6. Initialize the necessary variables like yolo_anchors, yolo_masks, yolo_max_boxes, batch_size, image_Size, num_Classes, class_dict that are useful for training.\n",
    "7. Define yolov3 architecture that is implemented compleyetly in kears and tensorflow.\n",
    "8. Define your own custom loss along with the yolo loss for object detection. I used focal loss for my task.\n",
    "9. Define parse_Dataset function which will help us the map the data that is needed for trainng by converting it into the format that yolov3 expects.\n",
    "10. Load both train and validation datasets.\n",
    "11. Initialize the model and load pretrained weights and freeze the layers that are not required for fine tuning.\n",
    "12. Start the trainnig. I used gradient tape method to train the model as it gives control over nitty gritty details in the model.\n",
    "13. While trainng make sure that we are saving logs and model weights for future use.\n",
    "14. Once the model is trained save the model weights instead of checkpoints.\n",
    "15. For making predictions, define yolo architecute again and load the saved weights and make the predictions and visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Goli_sbVHLDS",
    "papermill": {
     "duration": 0.458191,
     "end_time": "2022-03-21T01:09:20.191211",
     "exception": false,
     "start_time": "2022-03-21T01:09:19.733020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjB8MtlgHNsL",
    "papermill": {
     "duration": 0.465601,
     "end_time": "2022-03-21T01:09:21.110125",
     "exception": false,
     "start_time": "2022-03-21T01:09:20.644524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. When using categorical **cross entropy loss** for negative samples i.e Bounding box for which there is no object is to high and loss for positive samples is too low for which our optimizers tries to lower negative samples loss as it's higher.Therefore, our model wasn't working well at detections.\n",
    "2. So to overcome the above hurdle we have tried **focal loss** which try to down weight **negative sample loss** and hence improve our model performace.\n",
    "3. For this model I have taken alpha parameter of **focal loss 0.85**, I have also tried 0.60,0.65,0.70,0.75,0.80,0.85 but at last 0.75 suited best for this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glwtL1h5HSGk",
    "papermill": {
     "duration": 0.504206,
     "end_time": "2022-03-21T01:09:22.072271",
     "exception": false,
     "start_time": "2022-03-21T01:09:21.568065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Future Scope or Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfH_s8IHHVwW",
    "papermill": {
     "duration": 0.472662,
     "end_time": "2022-03-21T01:09:23.197304",
     "exception": false,
     "start_time": "2022-03-21T01:09:22.724642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. In our model we have used YOLOv3 which is a really good object detection technique but at the time of making this case study we already have YOLOV5 which is state of the art model.\n",
    "2. So using YOLOV5 might get some better results as yolov5 is faster than yolov3 and its accuracy is reasonable and comparable too.\n",
    "3. We have only 3k images for training, and we all know that more the data more the model performance for deep learning models. So training to incorporate Augmentations might be helpful.\n",
    "4. There is a module called Albumenations that will help immensely while augmenting images with bounding boxes.\n",
    "5. Instead of yolo we can use other models like Faster RCNN which is build for object detection tasks.\n",
    "6. We can get some more images from internet and pseudo labelling them carefully and addding those images for training might help too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pjspHfPIV33",
    "papermill": {
     "duration": 0.453332,
     "end_time": "2022-03-21T01:09:24.113716",
     "exception": false,
     "start_time": "2022-03-21T01:09:23.660384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What I Gained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSGIrtTBIa3o",
    "papermill": {
     "duration": 0.452678,
     "end_time": "2022-03-21T01:09:25.016749",
     "exception": false,
     "start_time": "2022-03-21T01:09:24.564071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. By doing this case study I got to know every single thing that YOLOV3 does internally for object detections.\n",
    "2. By implementing YOLOV3 completely in keras or tensoflow helped me gain confidence in me so that anyone with good knowledge can build an research on their own using all the tools available. \n",
    "3. If loss fucntion is continous and differentiable we can always back porpagate the loss and minimize loss using the techniques we learned in deep learning.\n",
    "4. Finally I think i will be albe to implement my own versions of other models in keras or tensorflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29969.32935,
   "end_time": "2022-03-21T01:09:28.843141",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-20T16:49:59.513791",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
